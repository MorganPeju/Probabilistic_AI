{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INF8225 - TP3\n",
        "\n",
        "Team:\n",
        "*   Renaud Lespérance (1802867)\n",
        "*   Morgan Péju (2103232)\n",
        "\n",
        "Link colab :\n",
        "https://colab.research.google.com/drive/1RVMUcgWjEiwhrRSTYz_dJudvjhaiBH_7?usp=sharing"
      ],
      "metadata": {
        "id": "qBLTUESQsJ-L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmOFPR8VmUq"
      },
      "source": [
        "# Machine translation\n",
        "\n",
        "The goal of this TP is to build a machine translation model.\n",
        "You will be comparing the performance of three different architectures:\n",
        "* A vanilla RNN\n",
        "* A GRU-RNN\n",
        "* A transformer\n",
        "\n",
        "You are provided with the code to load and build the pytorch dataset,\n",
        "and the code for the training loop.\n",
        "You \"only\" have to code the architectures.\n",
        "Of course, the use of built-in torch layers such as `nn.GRU`, `nn.RNN` or `nn.Transformer`\n",
        "is forbidden, as there would be no exercise otherwise.\n",
        "\n",
        "The source sentences are in english and the target language is french.\n",
        "\n",
        "This is also for you the occasion to see what a basic machine learning pipeline looks like.\n",
        "Take a look at the given code, you might learn a lot!\n",
        "\n",
        "Do not forget to **select the runtime type as GPU!**\n",
        "\n",
        "**Sources**\n",
        "\n",
        "* Dataset: [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
        "\n",
        "<!---\n",
        "M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proc. of EAMT, pp. 261-268, Trento, Italy. pdf, bib. [paper](https://aclanthology.org/2012.eamt-1.60.pdf). [website](https://wit3.fbk.eu/2016-01).\n",
        "-->\n",
        "\n",
        "* The code is inspired by this [pytorch tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html).\n",
        "\n",
        "*This notebook is quite big, use the table of contents to easily navigate through it.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyCdlapMV8Hu"
      },
      "source": [
        "# Imports and data initializations\n",
        "\n",
        "We first download and parse the dataset. From the parsed sentences\n",
        "we can build the vocabularies and the torch datasets.\n",
        "The end goal of this section is to have an iterator\n",
        "that can yield the pairs of translated datasets, and\n",
        "where each sentences is made of a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "vLbVbH4lu4J0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJQfREvFUdoz"
      },
      "outputs": [],
      "source": [
        "!python3 -m spacy download en > /dev/null\n",
        "!python3 -m spacy download fr > /dev/null\n",
        "!pip install torchinfo > /dev/null\n",
        "!pip install einops > /dev/null\n",
        "!pip install wandb > /dev/null\n",
        "\n",
        "\n",
        "from itertools import takewhile\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torchtext.datasets import IWSLT2016\n",
        "\n",
        "import einops\n",
        "import wandb\n",
        "from torchinfo import summary\n",
        "\n",
        "# from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizers are objects that are able to divide a python string into a list of tokens (words, punctuations, special tokens...) as a list of strings.\n",
        "\n",
        "The special tokens are used for a particular reasons:\n",
        "* *\\<unk\\>*: Replace an unknown word in the vocabulary by this default token\n",
        "* *\\<pad\\>*: Virtual token used to as padding token so a batch of sentences can have a unique length\n",
        "* *\\<bos\\>*: Token indicating the beggining of a sentence in the target sequence\n",
        "* *\\<eos\\>*: Token indicating the end of a sentence in the target sequence"
      ],
      "metadata": {
        "id": "ppPj9CrnsSoW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxNpMbkvUfGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56428ba4-1546-483c-c942-5658d0f200c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-07 20:26:50--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.186.54, 104.21.92.44, 2606:4700:3033::ac43:ba36, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.186.54|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6532197 (6.2M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   6.23M  8.65MB/s    in 0.7s    \n",
            "\n",
            "2022-04-07 20:26:52 (8.65 MB/s) - ‘fra-eng.zip’ saved [6532197/6532197]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "173106\n"
          ]
        }
      ],
      "source": [
        "# Original dataset, but there's a bug on Colab with it\n",
        "# train, valid, _ = IWSLT2016(language_pair=('fr', 'en'))\n",
        "# train, valid = list(train), list(valid)\n",
        "\n",
        "# Another dataset, but it is too huge\n",
        "# !wget https://www.statmt.org/wmt14/training-monolingual-europarl-v7/europarl-v7.en.gz\n",
        "# !wget https://www.statmt.org/wmt14/training-monolingual-europarl-v7/europarl-v7.fr.gz\n",
        "# !gunzip europarl-v7.en.gz\n",
        "# !gunzip europarl-v7.fr.gz\n",
        "\n",
        "# with open('europarl-v7.en', 'r') as my_file:\n",
        "#     english = my_file.readlines()\n",
        "\n",
        "# with open('europarl-v7.fr', 'r') as my_file:\n",
        "#     french = my_file.readlines()\n",
        "\n",
        "# dataset = [\n",
        "#     (en, fr)\n",
        "#     for en, fr in zip(english, french)\n",
        "# ]\n",
        "# print(f'\\n{len(dataset):,} sentences.')\n",
        "\n",
        "# dataset, _ = train_test_split(dataset, test_size=0.8, random_state=0)  # Remove 80% of the dataset (it would be huge otherwise)\n",
        "# train, valid = train_test_split(dataset, test_size=0.2, random_state=0)  # Split between train and validation dataset\n",
        "\n",
        "# Our current dataset\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "\n",
        "\n",
        "df = pd.read_csv('fra.txt', sep='\\t', names=['english', 'french', 'attribution'])\n",
        "train = [(en, fr) for en, fr in zip(df['english'], df['french'])]\n",
        "train, valid = train_test_split(train, test_size=0.1, random_state=0)\n",
        "print(len(train))\n",
        "\n",
        "en_tokenizer, fr_tokenizer = get_tokenizer('spacy', language='en'), get_tokenizer('spacy', language='fr')\n",
        "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddZvN5FiK9u"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Functions and classes to build the vocabularies and the torch datasets.\n",
        "The vocabulary is an object able to transform a string token into the id (an int) of that token in the vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2dKQ6PvZC_U"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset: list,\n",
        "            en_vocab: Vocab,\n",
        "            fr_vocab: Vocab,\n",
        "            en_tokenizer,\n",
        "            fr_tokenizer,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.en_vocab = en_vocab\n",
        "        self.fr_vocab = fr_vocab\n",
        "        self.en_tokenizer = en_tokenizer\n",
        "        self.fr_tokenizer = fr_tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of examples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple:\n",
        "        \"\"\"Return a sample.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            index: Index of the sample.\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            en_tokens: English tokens of the sample, as a LongTensor.\n",
        "            fr_tokens: French tokens of the sample, as a LongTensor.\n",
        "        \"\"\"\n",
        "        # Get the strings\n",
        "        en_sentence, fr_sentence = self.dataset[index]\n",
        "\n",
        "        # To list of words\n",
        "        # We also add the beggining-of-sentence and end-of-sentence tokens\n",
        "        en_tokens = ['<bos>'] + self.en_tokenizer(en_sentence) + ['<eos>']\n",
        "        fr_tokens = ['<bos>'] + self.fr_tokenizer(fr_sentence) + ['<eos>']\n",
        "\n",
        "        # To list of tokens\n",
        "        en_tokens = self.en_vocab(en_tokens)  # list[int]\n",
        "        fr_tokens = self.fr_vocab(fr_tokens)\n",
        "\n",
        "        return torch.LongTensor(en_tokens), torch.LongTensor(fr_tokens)\n",
        "\n",
        "\n",
        "def yield_tokens(dataset, tokenizer, lang):\n",
        "    \"\"\"Tokenize the whole dataset and yield the tokens.\n",
        "    \"\"\"\n",
        "    assert lang in ('en', 'fr')\n",
        "    sentence_idx = 0 if lang == 'en' else 1\n",
        "\n",
        "    for sentences in dataset:\n",
        "        sentence = sentences[sentence_idx]\n",
        "        tokens = tokenizer(sentence)\n",
        "        yield tokens\n",
        "\n",
        "\n",
        "def build_vocab(dataset: list, en_tokenizer, fr_tokenizer, min_freq: int):\n",
        "    \"\"\"Return two vocabularies, one for each language.\n",
        "    \"\"\"\n",
        "    en_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, en_tokenizer, 'en'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    en_vocab.set_default_index(en_vocab['<unk>'])  # Default token for unknown words\n",
        "\n",
        "    fr_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, fr_tokenizer, 'fr'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
        "\n",
        "    return en_vocab, fr_vocab\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "        dataset: list,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        max_words: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Preprocess the dataset.\n",
        "    Remove samples where at least one of the sentences are too long.\n",
        "    Those samples takes too much memory.\n",
        "    Also remove the pending '\\n' at the end of sentences.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "\n",
        "    for en_s, fr_s in dataset:\n",
        "        if len(en_tokenizer(en_s)) >= max_words or len(fr_tokenizer(fr_s)) >= max_words:\n",
        "            continue\n",
        "        \n",
        "        en_s = en_s.replace('\\n', '')\n",
        "        fr_s = fr_s.replace('\\n', '')\n",
        "\n",
        "        filtered.append((en_s, fr_s))\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def build_datasets(\n",
        "        max_sequence_length: int,\n",
        "        min_token_freq: int,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        train: list,\n",
        "        val: list,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Build the training, validation and testing datasets.\n",
        "    It takes care of the vocabulary creation.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        - max_sequence_length: Maximum number of tokens in each sequences.\n",
        "            Having big sequences increases dramatically the VRAM taken during training.\n",
        "        - min_token_freq: Minimum number of occurences each token must have\n",
        "            to be saved in the vocabulary. Reducing this number increases\n",
        "            the vocabularies's size.\n",
        "        - en_tokenizer: Tokenizer for the english sentences.\n",
        "        - fr_tokenizer: Tokenizer for the french sentences.\n",
        "        - train and val: List containing the pairs (english, french) sentences.\n",
        "\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        - (train_dataset, val_dataset): Tuple of the two TranslationDataset objects.\n",
        "    \"\"\"\n",
        "    datasets = [\n",
        "        preprocess(samples, en_tokenizer, fr_tokenizer, max_sequence_length)\n",
        "        for samples in [train, val]\n",
        "    ]\n",
        "\n",
        "    en_vocab, fr_vocab = build_vocab(datasets[0], en_tokenizer, fr_tokenizer, min_token_freq)\n",
        "\n",
        "    datasets = [\n",
        "        TranslationDataset(samples, en_vocab, fr_vocab, en_tokenizer, fr_tokenizer)\n",
        "        for samples in datasets\n",
        "    ]\n",
        "\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWlH-qEbkoYA"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch: list, src_pad_idx: int, tgt_pad_idx: int) -> tuple:\n",
        "    \"\"\"Add padding to the given batch so that all\n",
        "    the samples are of the same size.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        data_batch: List of samples.\n",
        "            Each sample is a tuple of LongTensors of varying size.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "        en_batch: Batch of tokens for the padded english sentences.\n",
        "            Shape of [batch_size, max_en_len].\n",
        "        fr_batch: Batch of tokens for the padded french sentences.\n",
        "            Shape of [batch_size, max_fr_len].\n",
        "    \"\"\"\n",
        "    en_batch, fr_batch = [], []\n",
        "    for en_tokens, fr_tokens in data_batch:\n",
        "        en_batch.append(en_tokens)\n",
        "        fr_batch.append(fr_tokens)\n",
        "\n",
        "    en_batch = pad_sequence(en_batch, padding_value=src_pad_idx, batch_first=True)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=tgt_pad_idx, batch_first=True)\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Gs4Myjh-jV"
      },
      "source": [
        "# Models architecture\n",
        "This is where you have to code the architectures.\n",
        "\n",
        "In a machine translation task, the model takes as input the whole\n",
        "source sentence along with the current known tokens of the target,\n",
        "and predict the next token in the target sequence.\n",
        "This means that the target tokens are predicted in an autoregressive\n",
        "manner, starting from the first token (right after the *\\<bos\\>* token) and producing tokens one by one until the last *\\<eos\\>* token.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "The loss is simply a *cross entropy loss* over the whole steps, where each class is a token of the vocabulary.\n",
        "\n",
        "![RNN schema for machinea translation](https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-translation-model-with-encoder-decoder-rnn.jpg)\n",
        "\n",
        "Note that in this image the english sentence is provided in reverse. \n",
        "\n",
        "---\n",
        "\n",
        "In pytorch, there is no dinstinction between an intermediate layer or a whole model having multiple layers in itself.\n",
        "Every layers or models inherit from the `torch.nn.Module`.\n",
        "This module needs to define the `__init__` method where you instanciate the layers,\n",
        "and the `forward` method where you decide how the inputs and the layers of the module interact between them.\n",
        "Thanks to the autograd computations of pytorch, you do not have\n",
        "to implement any backward method!\n",
        "\n",
        "A really important advice is to **always look at\n",
        "the shape of your input and your output.**\n",
        "From that, you can often guess how the layers should interact\n",
        "with the inputs to produce the right output.\n",
        "You can also easily detect if there's something wrong going on.\n",
        "\n",
        "You are more than advised to use the `einops` library and the `torch.einsum` function. This will require less operations than 'classical' code, but note that it's a bit trickier to use.\n",
        "This is a way of describing tensors manipulation with strings, bypassing the multiple tensor methods executed in the background.\n",
        "You can find a nice presentation of `einops` [here](https://einops.rocks/1-einops-basics/).\n",
        "A paper has just been released about einops [here](https://paperswithcode.com/paper/einops-clear-and-reliable-tensor).\n",
        "\n",
        "**A great tutorial on pytorch can be found [here](https://stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html).**\n",
        "Spending 3 hours on this tutorial is *no* waste of time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN models"
      ],
      "metadata": {
        "id": "xodRThXg2DHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN and GRU\n"
      ],
      "metadata": {
        "id": "ZvfRVUKm1u8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell(nn.Module):\n",
        "    \"\"\"A single RNN layer.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.device = config['device']\n",
        "        self.Wih = nn.Linear(input_size, hidden_size,device=self.device)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size,device=self.device)\n",
        "        self.Dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"Go through all the sequence in x, iteratively updatating\n",
        "        the hidden state h.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "        y_t = []\n",
        "\n",
        "        for idx in range(seq_len):\n",
        "          # RNN cell propagation\n",
        "          h = torch.tanh(self.Wih(x[:,idx]) + self.Whh(h))\n",
        "\n",
        "          # Add dropout on the outputs of each RNNCell except for the last one\n",
        "          if idx != (seq_len-1):\n",
        "            h = self.Dropout(h)\n",
        "\n",
        "          # Keep the output for all indexes in the sequence.\n",
        "          y_t.append(h)\n",
        "\n",
        "        y = torch.stack(y_t,dim=1)\n",
        "        return y,h\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "    \"\"\"A single GRU layer.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.device = config['device']\n",
        "\n",
        "        # all in one nn.Linear, r , u , n  will be split after propagation\n",
        "        self.Wih = nn.Linear(input_size, 3*hidden_size,device=self.device)\n",
        "        self.Whh = nn.Linear(hidden_size,3*hidden_size,device=self.device)\n",
        "        self.Dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        y_t = []\n",
        "\n",
        "        for idx in range(seq_len):\n",
        "          ### GRU cell propagation\n",
        "\n",
        "          # propagation of input\n",
        "          x_input = self.Wih(x[:,idx])\n",
        "          x_r,x_u,x_n = x_input.chunk(3,1) #Split r,u,n \n",
        "\n",
        "          # propagation of hidden\n",
        "          hid = self.Whh(h)\n",
        "          h_r,h_u,h_n = hid.chunk(3,1) #Split r,u,n\n",
        "\n",
        "          # Gate activation\n",
        "          reset_g = torch.sigmoid(x_r+h_r)\n",
        "          update_g = torch.sigmoid(x_u+h_u)\n",
        "          new_g = torch.tanh(x_n + torch.mul(reset_g,h_n))\n",
        "\n",
        "          # Output of the GRUCell\n",
        "          h = torch.mul(new_g,(1-update_g)) + torch.mul(update_g, h)\n",
        "\n",
        "          # Add dropout on the outputs of each GRUCell except for the last one\n",
        "          if idx != (seq_len-1) :\n",
        "            h = self.Dropout(h)\n",
        "\n",
        "          # Keep the output for all indexes in the sequence.\n",
        "          y_t.append(h)\n",
        "\n",
        "        y = torch.stack(y_t,dim=1)\n",
        "        return y,h\n",
        "\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"Implementation of an RNN based\n",
        "    on https://pytorch.org/docs/stable/generated/torch.nn.RNN.html.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        num_layers: Number of layers (RNNCell or GRUCell).\n",
        "        dropout: Dropout rate.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            num_layers: int,\n",
        "            dropout: float,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.device = config['device']\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self._cells = nn.ModuleList()\n",
        "\n",
        "        ### First layer of the RNN\n",
        "        if model_type == 'RNN':\n",
        "          self._cells.append(RNNCell(input_size,hidden_size,dropout))\n",
        "        elif model_type == 'GRU':\n",
        "          self._cells.append(GRUCell(input_size,hidden_size,dropout))\n",
        "        else :\n",
        "          print(f\"model type {self.model_type} is not supported\")\n",
        "\n",
        "        ### Folowing layer if num_layers > 1\n",
        "        for i in range(num_layers-1):\n",
        "          if model_type == 'RNN':\n",
        "            self._cells.append(RNNCell(hidden_size,hidden_size,dropout))\n",
        "          elif model_type == 'GRU':\n",
        "            self._cells.append(GRUCell(hidden_size,hidden_size,dropout))\n",
        "          else :\n",
        "            print(f\"model type {self.model_type} is not supported\")\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor=None) -> tuple:\n",
        "        \"\"\"Pass the input sequence through all the RNN cells.\n",
        "        Returns the output and the final hidden state of each RNN layer\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Hidden state for each RNN layer.\n",
        "                Can be None, in which case an initial hidden state is created.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Output embeddings for each token after the RNN layers.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Final hidden state.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "        \"\"\"\n",
        "        # For the first layer, no input from a previous hidden layer is available\n",
        "        if h is None :\n",
        "          h = torch.zeros(x.shape[0],self.hidden_size,device=self.device)\n",
        "        else:\n",
        "          h = h[:,self.num_layers-1]\n",
        "\n",
        "        # Propagate one cell at a time\n",
        "        h_out=[]\n",
        "        for layer,cell in enumerate(self._cells):\n",
        "          if layer == 0 : \n",
        "            y,h = cell(x,h)\n",
        "          else:\n",
        "            y,h = cell(y,h)\n",
        "          h_out.append(h)\n",
        "\n",
        "        h_out = torch.stack(h_out,dim=1)\n",
        "\n",
        "        return y,h_out"
      ],
      "metadata": {
        "id": "RiNKnwScM5Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation RNN\n",
        "\n",
        "This module instanciates a vanilla RNN or a GRU-RNN and performs the translation task. You have to:\n",
        "* Encode the source and target sequence\n",
        "* Pass the final hidden state of the encoder to the decoder (one for each layer)\n",
        "* Decode the hidden state into the target sequence\n",
        "\n",
        "We use teacher forcing for training, meaning that when the next token is predicted, that prediction is based on the previous true target tokens. "
      ],
      "metadata": {
        "id": "boIetZUy1f-5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD-6N17xhuLy"
      },
      "outputs": [],
      "source": [
        "class TranslationRNN(nn.Module):\n",
        "    \"\"\"Basic RNN encoder and decoder for a translation task.\n",
        "    It can run as a vanilla RNN or a GRU-RNN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the hidden layers in the RNNs\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the RNNs.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "        torch_fct: If true use torch RNN or GRU else our model \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "            model_type: str,\n",
        "            torch_fct_translation: bool,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.device = config['device']\n",
        "\n",
        "        # Source and Target Embeddings\n",
        "        self.embedding_encoder = nn.Embedding(n_tokens_src, dim_embedding, padding_idx=src_pad_idx)\n",
        "        self.embedding_decoder = nn.Embedding(n_tokens_tgt, dim_embedding, padding_idx=tgt_pad_idx)\n",
        "\n",
        "        if torch_fct_translation : # Use torch functions\n",
        "          if model_type  == 'RNN' :\n",
        "            self.model_encoder = nn.RNN(dim_embedding, dim_hidden, n_layers, dropout=dropout, batch_first=True)\n",
        "            self.model_decoder = nn.RNN(dim_embedding, dim_hidden, n_layers, dropout=dropout, batch_first=True)\n",
        "          elif model_type == 'GRU' :\n",
        "            self.model_encoder = nn.GRU(dim_embedding, dim_hidden, n_layers, dropout=dropout, batch_first=True)\n",
        "            self.model_decoder = nn.GRU(dim_embedding, dim_hidden, n_layers, dropout=dropout, batch_first=True)\n",
        "          else :\n",
        "            print(f\"model type {self.model_type} is not supported\")\n",
        "        else : # Use our functions\n",
        "          if model_type  == 'RNN' :\n",
        "            self.model_encoder = RNN(dim_embedding, dim_hidden, n_layers, dropout,model_type='RNN')\n",
        "            self.model_decoder = RNN(dim_embedding, dim_hidden, n_layers, dropout,model_type='RNN')\n",
        "          elif model_type == 'GRU' :\n",
        "            self.model_encoder = RNN(dim_embedding, dim_hidden, n_layers, dropout,model_type='GRU')\n",
        "            self.model_decoder = RNN(dim_embedding, dim_hidden, n_layers, dropout,model_type='GRU')\n",
        "          else :\n",
        "            print(f\"model type {self.model_type} is not supported\")\n",
        "\n",
        "        # Add normalization layer between encoder output and decoder input\n",
        "        self.LNorm = nn.LayerNorm(dim_hidden,device=self.device)\n",
        "\n",
        "        ### Uses an MLP for the translator output instead to increase accuracy.\n",
        "        MLP_param = config['MLP_param_RNN_GRU']\n",
        "        MLP_act = MLP_param[0] # Activation function to use\n",
        "        if MLP_act == \"LeakyReLU01\":\n",
        "          act = nn.LeakyReLU(0.1)\n",
        "        if MLP_act == \"ELU\":\n",
        "          act = nn.ELU()\n",
        "        if MLP_act == \"Mish\":\n",
        "          act = nn.Mish()\n",
        "\n",
        "        MLP_layers = MLP_param[1] # Number of layers in our MLP\n",
        "        MLP_scale = MLP_param[2]\n",
        "        layers = []\n",
        "        dropout_l = nn.Dropout(dropout)\n",
        "        if MLP_layers == 1 :\n",
        "          layers.append(nn.Linear(dim_hidden, n_tokens_tgt,device=self.device))\n",
        "        else: \n",
        "          #First layer\n",
        "          layers.append(nn.Linear(dim_hidden, dim_hidden*MLP_scale,device=self.device))\n",
        "          layers.append(dropout_l)\n",
        "          layers.append(act)\n",
        "          layers.append(nn.LayerNorm((dim_hidden*MLP_scale),device=self.device))\n",
        "          \n",
        "          for idx in range(MLP_layers-1) :\n",
        "            if idx == MLP_layers-2 : #last hidden layer\n",
        "              layers.append(nn.Linear(dim_hidden*MLP_scale, n_tokens_tgt,device=self.device))\n",
        "            else:\n",
        "              layers.append(nn.Linear(dim_hidden*MLP_scale, dim_hidden*MLP_scale,device=self.device))\n",
        "              layers.append(dropout_l)\n",
        "              layers.append(act)\n",
        "              layers.append(nn.LayerNorm((dim_hidden*MLP_scale),device=self.device))\n",
        "\n",
        "        #Create object _sequential to propagate MLP in one call\n",
        "        self._sequential = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the source tokens based on the target tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "        \n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, tgt_seq_len, n_tokens_tgt].\n",
        "        \"\"\"  \n",
        "        ## ENCODER\n",
        "        encoder_embedding = self.embedding_encoder(source)  # [batch_size, src_seq_len, embedding dim])\n",
        "\n",
        "        ## Application of the RNN or GRU on the encoded source data.\n",
        "        # If we initialize no \"hidden layer\", model_encoder automatically initializes the 1st hidden with zeros\n",
        "        outputs, hidden = self.model_encoder(encoder_embedding) # output: [batch_size, src_seq_len, hidden dim] ;; hidden: [n_layers, batch_size, hidden_dim]\n",
        "        hidden = self.LNorm(hidden)\n",
        "\n",
        "        ## DECODER\n",
        "        decoder_embedding = self.embedding_decoder(target) # [1, batch_size, embeddding dim]\n",
        "        predict, hidden = self.model_decoder(decoder_embedding, hidden)\n",
        "\n",
        "        ### returns the logits after a MLP\n",
        "        return  self._sequential(predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer model\n",
        "Here you have to code the Transformer architecture.\n",
        "It is divided in three parts:\n",
        "* Attention layers\n",
        "* Encoder and decoder layers\n",
        "* Main layers (gather the encoder and decoder layers)\n",
        "\n",
        "The [illustrated transformer](https://jalammar.github.io/illustrated-transformer/) blog can help you\n",
        "understanding how the architecture works.\n",
        "Once this is done, you can use [the annontated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) to have an idea of how to code this architecture.\n",
        "We encourage you to use `torch.einsum` and the `einops` library as much as you can. It will make your code simpler.\n",
        "\n",
        "---\n",
        "**Implementation order**\n",
        "\n",
        "To help you with the implementation, we advise you following this order:\n",
        "* Implement `TranslationTransformer` and use `nn.Transformer` instead of `Transformer`\n",
        "* Implement `Transformer` and use `nn.TransformerDecoder` and `nn.TransformerEnocder`\n",
        "* Implement the `TransformerDecoder` and `TransformerEncoder` and use `nn.MultiHeadAttention`\n",
        "* Implement `MultiHeadAttention`\n",
        "\n",
        "Do not forget to add `batch_first=True` when necessary in the `nn` modules."
      ],
      "metadata": {
        "id": "EZcGlRnZvOnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention layers\n",
        "We use a `MultiHeadAttention` module, that is able to perform self-attention aswell as cross-attention (depending on what you give as queries, keys and values).\n",
        "\n",
        "**Attention**\n",
        "\n",
        "\n",
        "It takes the multiheaded queries, keys and values as input.\n",
        "It computes the attention between the queries and the keys and return the attended values.\n",
        "\n",
        "The implementation of this function can greatly be improved with *einsums*.\n",
        "\n",
        "**MultiheadAttention**\n",
        "\n",
        "Computes the multihead queries, keys and values and feed them to the `attention` function.\n",
        "You also need to merge the key padding mask and the attention mask into one mask.\n",
        "\n",
        "The implementation of this module can greatly be improved with *einops.rearrange*."
      ],
      "metadata": {
        "id": "OFxV-6M3402p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "from torch._C import device\n",
        "\n",
        "def attention(\n",
        "        q: torch.FloatTensor,\n",
        "        k: torch.FloatTensor,\n",
        "        v: torch.FloatTensor,\n",
        "        mask: torch.BoolTensor=None,\n",
        "        dropout: nn.Dropout=None,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Computes multihead scaled dot-product attention from the\n",
        "    projected queries, keys and values.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        q: Batch of queries.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        k: Batch of keys.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        v: Batch of values.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        mask: Prevent tokens to attend to some other tokens (for padding or autoregressive attention).\n",
        "            Attention is prevented where the mask is `True`.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2],\n",
        "            or broadcastable to that shape.\n",
        "        dropout: Dropout layer to use.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        y: Multihead scaled dot-attention between the queries, keys and values.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        attn: Computed attention mask.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2].\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    # Définition des variables\n",
        "    batch_size = q.shape[0]\n",
        "    k_length = k.shape[1]\n",
        "    d_model = q.shape[3]\n",
        "    n_heads = q.shape[2]\n",
        "    d_k = d_model\n",
        "\n",
        "    \n",
        "    # Permutations\n",
        "    q = q.permute(0,2,1,3)\n",
        "    k = k.permute(0,2,1,3)\n",
        "    v = v.permute(0,2,1,3)\n",
        "    # Scaling pour éviter la saturation de softmax\n",
        "    scaled_attention = q / np.sqrt(d_k)  # [batch_size, n_heads, seq_len_1, dim_per_head]\n",
        "    \n",
        "    # Calcul des scores qui déterminent l'importance à accorder aux autres parties de la phrase d'entrée lorsqu'on encode un mot à une certaine position\n",
        "    scores = torch.matmul(scaled_attention, k.transpose(2,3))   # [batch_size, n_heads, seq_len_1, seq_len_2]\n",
        "    \n",
        "    # Application du mask\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill_(mask==1,float(\"-inf\"))\n",
        "    \n",
        "    # Application de Softmax pour normaliser les scores : déterminer le score pour lequel chaque mot sera exprimé à cette position\n",
        "    attn = nn.Softmax(dim=-1)(scores)   # [batch_size, n_heads, seq_len_1, seq_len_2]\n",
        "    # Application d'un dropout\n",
        "    scores = dropout(scores) if dropout is not None else scores\n",
        "\n",
        "    # Multiplier \"value\" par les scores pour obtenir les valeurs des mots sur lesquels on veut se concentrer et réduire les mots moins pertinents \n",
        "    # Et les sommer pour obtenir la sortie de la couche d'attention\n",
        "    y = torch.matmul(attn, v)   \n",
        "   \n",
        "    y = y.permute(0,2,1,3) # Permutation pour avoir le format souhaité [batch_size, seq_len_1, n_heads, dim_model]\n",
        "    '''\n",
        "    scores = torch.einsum(\"blhk,bthk->bhlt\",[q,k])\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill_(mask==1, float('-inf'))\n",
        "      \n",
        "    attn = torch.softmax(scores/np.sqrt(k.shape[0]), dim=3)\n",
        "    y = torch.einsum(\"bhlt,bthk->blhk\", [dropout(attn),v])\n",
        "    '''\n",
        "    return y, attn\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multihead attention module.\n",
        "    Can be used as a self-attention and cross-attention layer.\n",
        "    The queries, keys and values are projected into multiple heads\n",
        "    before computing the attention between those tensors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        dim: Dimension of the input tokens.\n",
        "        n_heads: Number of heads. `dim` must be divisible by `n_heads`.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim: int,\n",
        "            n_heads: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert dim % n_heads == 0 #\"Embedding dimension must be 0 modulo number of heads.\"\n",
        "        \n",
        "        # TODO\n",
        "        # Définition des variables\n",
        "        self.device = config['device']\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        # Définition des projections pour \"query\",\"key\" et \"value\"\n",
        "        self.Wq = nn.Linear(dim, dim,device=self.device)\n",
        "        self.Wk = nn.Linear(dim, dim,device=self.device)\n",
        "        self.Wv = nn.Linear(dim, dim,device=self.device)\n",
        "        # Définition de la couche \"fully connected\"\n",
        "        self.fc = nn.Linear(dim, dim,device=self.device)\n",
        "        # Définition du dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(\n",
        "            self,\n",
        "            q: torch.FloatTensor,\n",
        "            k: torch.FloatTensor,\n",
        "            v: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor = None,\n",
        "            attn_mask: torch.BoolTensor = None,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Computes the scaled multi-head attention form the input queries,\n",
        "        keys and values.\n",
        "\n",
        "        Project those queries, keys and values before feeding them\n",
        "        to the `attention` function.\n",
        "\n",
        "        The masks are boolean masks. Tokens are prevented to attends to\n",
        "        positions where the mask is `True`.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            q: Batch of queries.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "            k: Batch of keys.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            v: Batch of values.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            key_padding_mask: Prevent attending to padding tokens.\n",
        "                Shape of [batch_size, seq_len_2].\n",
        "            attn_mask: Prevent attending to subsequent tokens.\n",
        "                Shape of [seq_len_1, seq_len_2].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Computed multihead attention.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        batch_size = q.shape[0]\n",
        "        seq_len_1, seq_len_2 = q.shape[1], k.shape[1]\n",
        "        \n",
        "        # Projections de \"query\",\"key\" et \"value\"\n",
        "        q = self.Wq(q)  # [batch_size, seq_len1, d_model]\n",
        "        k = self.Wk(k)  # [batch_size, seq_len2, d_model]\n",
        "        v = self.Wv(v)  # [batch_size, seq_len2, d_model]\n",
        "        \n",
        "        # Reshape        \n",
        "        q = q.view(batch_size, -1, self.n_heads, self.dim//self.n_heads) # [batch_size, n_heads, seq_len_1, depth]\n",
        "        k = k.view(batch_size, -1, self.n_heads, self.dim//self.n_heads)  # [batch_size, n_heads, seq_len_2, depth]   \n",
        "        v = v.view(batch_size, -1, self.n_heads, self.dim//self.n_heads) # [batch_size, n_heads, seq_len_2, depth]\n",
        "                   \n",
        "        # Gestion des masks key padding et attention masks\n",
        "        \n",
        "        if key_padding_mask is not None:\n",
        "            # Reshape du mask de padding [batch_size, 1, 1, seq_len_2]\n",
        "            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, seq_len_2)\n",
        "            if attn_mask is not None: # Si on a mask d'attention et de padding\n",
        "              attn_mask = attn_mask.view(1, 1, seq_len_1, seq_len_2)\n",
        "              mask = torch.logical_or(attn_mask>0,key_padding_mask) \n",
        "            else: # Si on a uniquement mask de padding\n",
        "              mask = key_padding_mask\n",
        "              \n",
        "            \n",
        "        # Appel à la fonction \"scaled dot product\" pour le calcul de l'attention\n",
        "        # scaled_attention shape [batch_size, seq_len_1, n_heads, depth]\n",
        "        # attention_weights shape [batch_size, n_heads, seq_len_1, seq_len_2]\n",
        "        scaled_attention, attention_weights = attention(q, k, v, mask=mask)\n",
        "        # Reshape de l'attention : [batch_size, seq_len_1, d_model]\n",
        "        scaled_attention = scaled_attention.contiguous().view(batch_size, -1, self.n_heads * (self.dim // self.n_heads))  \n",
        "        \n",
        "        # Application d'une couche \"fully connected\"\n",
        "        y = self.fc(scaled_attention) # y shape [batch_size, seq_len_1, dim_model]\n",
        "        \n",
        "        return y\n"
      ],
      "metadata": {
        "id": "A0jOZxOwu_Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder and decoder layers\n",
        "\n",
        "**TranformerEncoder**\n",
        "\n",
        "Apply self-attention layers onto the source tokens.\n",
        "It only needs the source key padding mask.\n",
        "\n",
        "\n",
        "**TranformerDecoder**\n",
        "\n",
        "Apply masked self-attention layers to the target tokens and cross-attention\n",
        "layers between the source and the target tokens.\n",
        "It needs the source and target key padding masks, and the target attention mask."
      ],
      "metadata": {
        "id": "nIpHjOtK47DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import ModuleList\n",
        "from torch._C import device\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Single decoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "            torch_fct_transformer: bool,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        # Définition des variables\n",
        "        self.device = config['device']\n",
        "        self.torch_fct_transformer = torch_fct_transformer\n",
        "\n",
        "        # Définition de l'appel à la couche d'attention\n",
        "        if torch_fct_transformer[3]== True: # Utilisation de Pytorch pour MultiheadAttention\n",
        "          self.selfAttention = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first = True,device=self.device)\n",
        "          self.multiheadAttention = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first = True,device=self.device)\n",
        "        elif torch_fct_transformer[3]== False: # Utilisation du MultiheadAttention \"Homemade\"\n",
        "          self.selfAttention = MultiheadAttention(d_model, nhead, dropout)\n",
        "          self.multiheadAttention = MultiheadAttention(d_model, nhead, dropout)\n",
        "\n",
        "        # Définition des couches du décodeur\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff,device=self.device),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model,device=self.device)\n",
        "        )\n",
        "        # Définition de couches de normalisation\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model,device=self.device)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model,device=self.device)\n",
        "        self.layer_norm3 = nn.LayerNorm(d_model,device=self.device)\n",
        "        # Définition des dropouts\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(\n",
        "            self,\n",
        "            tgt: torch.FloatTensor,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decode the next target tokens based on the previous tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            tgt: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            src: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        y = tgt\n",
        "        # Appel à la couche de \"self attention\" (Pytorch ou \"Homemade\")\n",
        "        if self.torch_fct_transformer[3]== True : # Utilisation de Pytorch pour MultiheadAttention\n",
        "          attn1, _ = self.selfAttention(y, y, y, attn_mask=tgt_mask_attn, key_padding_mask=tgt_key_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "          attn1 = self.dropout1(attn1)\n",
        "        elif self.torch_fct_transformer[3]== False: # Utilisation du MultiheadAttention \"Homemade\"\n",
        "          attn1 = self.dropout1(self.selfAttention(y, y, y, attn_mask=tgt_mask_attn, key_padding_mask=tgt_key_padding_mask))  # (batch_size, target_seq_len, d_model)\n",
        "       \n",
        "        # Application d'une normalisation\n",
        "        y = self.layer_norm1(y + attn1)\n",
        "\n",
        "        # Appel à la couche d'attention (Pytorch ou \"Homemade\")\n",
        "        if self.torch_fct_transformer[3]== True: # Utilisation de Pytorch pour MultiheadAttention\n",
        "          attn2, _ = self.multiheadAttention(y, src, src, key_padding_mask=src_key_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "          attn2 = self.dropout1(attn2)\n",
        "        elif self.torch_fct_transformer[3]== False: #Utilisation du MultiheadAttention \"Homemade\"\n",
        "          attn2 = self.dropout2(self.multiheadAttention(y, src, src, key_padding_mask=src_key_padding_mask))  # (batch_size, target_seq_len, d_model)\n",
        "        # Application d'une normalisation\n",
        "        y = self.layer_norm2(y + attn2)\n",
        "        \n",
        "        # Forward dans les couches du décodeur\n",
        "        ffn_output = self.dropout3(self.feed_forward(y))      \n",
        "        # Application d'une normalisation\n",
        "        y = self.layer_norm3(y + ffn_output)\n",
        "        return y\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer decoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_decoder_layers: Number of stacked decoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            num_decoder_layer:int ,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "            torch_fct_transformer: bool,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        # Définition des variables\n",
        "        self.device = config['device']\n",
        "        self.d_model = d_model\n",
        "        self.num_decoder_layer = num_decoder_layer\n",
        "        # Définition des couches du décodeur\n",
        "        self.dec_layers = [TransformerDecoderLayer(d_model, d_ff, nhead, dropout, torch_fct_transformer) \n",
        "                          for _ in range(num_decoder_layer)]\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(d_model, device=self.device)\n",
        "    def forward(\n",
        "            self,\n",
        "            tgt: torch.FloatTensor,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decodes the source sequence by sequentially passing.\n",
        "        the encoded source sequence and the target sequence through the decoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            tgt: Batch of taget sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            src: Batch of encoded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        y = tgt\n",
        "        # Appel aux couches du décodeur\n",
        "        for i in range(self.num_decoder_layer):\n",
        "          y = self.dec_layers[i](y, src, tgt_mask_attn, src_key_padding_mask,tgt_key_padding_mask)\n",
        "\n",
        "        y = self.layer_norm(y)\n",
        "       \n",
        "        return y # shape [batch_size, tgt_seq_len, d_model]\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Single encoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of input tokens.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "            torch_fct_transformer: bool,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        # Définition des variables\n",
        "        self.device = config['device']\n",
        "        self.torch_fct_transformer = torch_fct_transformer\n",
        "\n",
        "        # Définition de la couche d'attention (Pytorch ou \"Homemade\")\n",
        "        if torch_fct_transformer[3]== True: # Utilisation de Pytorch pour MultiheadAttention\n",
        "          self.multiheadAttention = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first = True,device=self.device)\n",
        "        elif torch_fct_transformer[3]== False: # Utilisation du MultiheadAttention \"homemade\"\n",
        "          self.multiheadAttention = MultiheadAttention(d_model, nhead, dropout)\n",
        "        # Définition des couches de l'encodeur\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff,device=self.device),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model,device=self.device)\n",
        "        )\n",
        "\n",
        "        # Définition de la normalisation des couches\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model,device=self.device)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model,device=self.device)\n",
        "        # Définitions des dropouts\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.FloatTensor,\n",
        "        key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the input. Does not attend to masked inputs.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Appel à la couche d'attention (Pytorch ou \"Homemade\")\n",
        "        if self.torch_fct_transformer[3]== True: # Utilisation de Pytorch pour MultiheadAttention\n",
        "          attn_output, _ = self.multiheadAttention(src, src, src, key_padding_mask=key_padding_mask)  # [batch_size, src_seq_len, d_model]\n",
        "        elif self.torch_fct_transformer[3]== False: # Utilisation du MultiheadAttention \"Homemade\"\n",
        "          attn_output = self.multiheadAttention(src, src, src, key_padding_mask=key_padding_mask)  # [batch_size, src_seq_len, d_model]\n",
        "        \n",
        "        # Application d'un dropout\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        # Application de la couche de normalisation\n",
        "        out1 = self.layer_norm1(src + attn_output)  # [batch_size, src_seq_len, d_model]\n",
        "        \n",
        "        # Forward dans les couches de l'encodeur\n",
        "        ffn_output = self.feed_forward(out1)  # [batch_size, src_seq_len, d_model]\n",
        "        # Application d'un dropout\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        # Application de la couche de normalisation pour obtenir le batch de token encodés de sortie de couche d'encodeur\n",
        "        y = self.layer_norm2(out1 + ffn_output)  # [batch_size, src_seq_len, d_model]\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer encoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders inputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            dim_feedforward: int,\n",
        "            num_encoder_layers: int,\n",
        "            nheads: int,\n",
        "            dropout: float,\n",
        "            torch_fct_transformer: bool,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        # Définition des variables\n",
        "        self.device = config['device']\n",
        "        self.d_model = d_model\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "\n",
        "        # Définition des couches de l'encodeur\n",
        "        self.enc_layers = [TransformerEncoderLayer(d_model,dim_feedforward,nheads, dropout, torch_fct_transformer) \n",
        "                            for _ in range(num_encoder_layers)]\n",
        "        self.layer_norm = nn.LayerNorm(d_model, device=self.device)\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the source sequence by sequentially passing.\n",
        "        the source sequence through the encoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source sequence.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        y = src\n",
        "        # Appel aux couches de l'encodeur\n",
        "        for i in range(self.num_encoder_layers) :\n",
        "          y = self.enc_layers[i](y, key_padding_mask)\n",
        "        \n",
        "        y = self.layer_norm(y) \n",
        "        return y  # [batch_size, src_seq_len, d_model]"
      ],
      "metadata": {
        "id": "2d-ukpIOu_RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main layers\n",
        "This section gather the `Transformer` and the `TranslationTransformer` modules.\n",
        "\n",
        "**Transformer**\n",
        "\n",
        "\n",
        "The classical transformer architecture.\n",
        "It takes the source and target tokens embeddings and\n",
        "do the forward pass through the encoder and decoder.\n",
        "\n",
        "**Translation Transformer**\n",
        "\n",
        "Compute the source and target tokens embeddings, and apply a final head to produce next token logits.\n",
        "The output must not be the softmax but just the logits, because we use the `nn.CrossEntropyLoss`.\n",
        "\n",
        "It also creates the *src_key_padding_mask*, the *tgt_key_padding_mask* and the *tgt_mask_attn*."
      ],
      "metadata": {
        "id": "Gd3kGoRO4_TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## EXPERIMENT - Positional Encoding\n",
        "\n",
        "class PositionalEncoding_Experiment(nn.Module):\n",
        "    \"\"\"\n",
        "    Compute positional embedding with sinusoid\n",
        "\n",
        "    Inspired by : https://github.com/hyunwoongko/transformer/blob/master/README.md\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len, device):\n",
        "        \"\"\"\n",
        "        d_model: dimension of model\n",
        "        max_len: max sequence length\n",
        "        device: device setting\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding_Experiment, self).__init__()\n",
        "\n",
        "        # Initialization of the positional embedding\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        self.encoding.requires_grad = False  \n",
        "\n",
        "        pos = torch.arange(0, max_len, device=device)\n",
        "        pos = pos.float().unsqueeze(dim=1)\n",
        "\n",
        "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
        "\n",
        "        # Compute positional embeddings\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "\n",
        "        return self.encoding[:seq_len, :]"
      ],
      "metadata": {
        "id": "Ur6yNPgTYHFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import device\n",
        "from IPython.lib.display import YouTubeVideo\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"Implementation of a Transformer based on the paper: https://arxiv.org/pdf/1706.03762.pdf.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders/decoders inputs/ouputs.\n",
        "        nhead: Number of heads for each multi-head attention.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        num_decoder_layers: Number of stacked encoders.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            nhead: int,\n",
        "            num_encoder_layers: int,\n",
        "            num_decoder_layers: int,\n",
        "            dim_feedforward: int,\n",
        "            dropout: float,\n",
        "            torch_fct_transformer: bool,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "        # Définition des variables\n",
        "        self.device = config['device']\n",
        "        self.torch_fct_transformer = torch_fct_transformer\n",
        "\n",
        "        # Définition de l'encodeur (Pytorch ou \"Homemade\")\n",
        "        if torch_fct_transformer[1] == True: # Utilisation de Pytorch pour l'encodeur\n",
        "          encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True,device=self.device)\n",
        "          self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        elif torch_fct_transformer[1] == False: # Utilisation de l'encodeur \"homemade\"\n",
        "          self.transformer_encoder = TransformerEncoder(d_model, dim_feedforward=dim_feedforward, num_encoder_layers=num_encoder_layers, nheads=nhead, dropout=dropout, torch_fct_transformer=torch_fct_transformer)\n",
        "       \n",
        "        # Définition du décodeur (Pytorch ou \"Homemade\")\n",
        "        if torch_fct_transformer[2] == True: # Utilisation de Pytorch pour le decodeur\n",
        "          decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True,device=self.device)\n",
        "          self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "        elif torch_fct_transformer[2] == False: # Utilisation du décodeur \"homemade\"\n",
        "          self.transformer_decoder = TransformerDecoder(d_model, d_ff=dim_feedforward, num_decoder_layer=num_decoder_layers, nhead=nhead, dropout=dropout, torch_fct_transformer=torch_fct_transformer)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Compute next token embeddings.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sequences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model]. \n",
        "            tgt: Batch of target sequences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Next token embeddings, given the previous target tokens and the source tokens.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Appel à l'encodeur (Pytorch ou \"Homemade\")\n",
        "        if self.torch_fct_transformer[1] == True: # Utilisation de l'encoder de Pytorch\n",
        "          memory = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
        "        else: # Utilisation de l'encoder \"homemade\"\n",
        "          memory = self.transformer_encoder(src, key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Appel au decodeur (Pytorch ou \"Homemade\")\n",
        "        if self.torch_fct_transformer[2] == True: # Utilisation du décodeur de Pytorch\n",
        "          y = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask_attn, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask)\n",
        "        else: # Utilisation du décodeur \"homemade\"\n",
        "          y = self.transformer_decoder(tgt, memory, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "\n",
        "        return y # y shape [batch_size, tgt_seq_len, dim_model]\n",
        "\n",
        "\n",
        "class TranslationTransformer(nn.Module):\n",
        "    \"\"\"Basic Transformer encoder and decoder for a translation task.\n",
        "    Manage the masks creation, and the token embeddings.\n",
        "    Position embeddings can be learnt with a standard `nn.Embedding` layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        n_heads: Number of heads for each multi-head attention.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the feedforward layers\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the encoder and decoder.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            n_heads: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "            torch_fct_transformer: bool,\n",
        "            positional_embeddings_exp: bool\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        # Définition des variables\n",
        "        self.device = config['device']\n",
        "        self.dim_embedding = dim_embedding\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.torch_fct_transformer = torch_fct_transformer\n",
        "\n",
        "        # Définition des embeddings\n",
        "        self.embedding_src = nn.Embedding(n_tokens_src, dim_embedding, padding_idx=src_pad_idx,device=self.device)\n",
        "        self.embedding_tgt = nn.Embedding(n_tokens_tgt, dim_embedding, padding_idx=tgt_pad_idx,device=self.device)\n",
        "        self.embedding_pos_src = nn.Embedding(n_tokens_src, dim_embedding, padding_idx=src_pad_idx,device=self.device)\n",
        "        self.embedding_pos_tgt = nn.Embedding(n_tokens_tgt, dim_embedding, padding_idx=tgt_pad_idx,device=self.device)\n",
        "        self.dropout_enc = nn.Dropout(dropout)\n",
        "        self.dropout_dec = nn.Dropout(dropout)\n",
        "        # Définition de la couche fully connected\n",
        "        self.fc_linear = nn.Linear(dim_embedding, n_tokens_tgt,device=self.device)\n",
        "        # Définition du transformer (Pytorch ou \"homemade\")\n",
        "        if torch_fct_transformer[0]:\n",
        "          self.transformer_model = nn.Transformer(d_model=dim_embedding,nhead=n_heads, num_encoder_layers=n_layers,num_decoder_layers=n_layers, dim_feedforward=dim_hidden, dropout=dropout, batch_first=True,device=self.device)\n",
        "        else:\n",
        "          self.transformer_model = Transformer(d_model=dim_embedding, nhead=n_heads, num_encoder_layers=n_layers,num_decoder_layers=n_layers, dim_feedforward=dim_hidden, dropout=dropout, torch_fct_transformer=torch_fct_transformer)\n",
        "\n",
        "        ### Uses an MLP for the translator output instead to increase accuracy.\n",
        "        MLP_param = config['MLP_param_transformer']\n",
        "        MLP_act = MLP_param[0] # Activation function to use\n",
        "        if MLP_act == \"LeakyReLU01\":\n",
        "          act = nn.LeakyReLU(0.1)\n",
        "        if MLP_act == \"ELU\":\n",
        "          act = nn.ELU()\n",
        "        if MLP_act == \"Mish\":\n",
        "          act = nn.Mish()\n",
        "\n",
        "        MLP_layers = MLP_param[1] # Number of layers in our MLP\n",
        "        MLP_scale = MLP_param[2]\n",
        "        layers = []\n",
        "        dropout_l = nn.Dropout(dropout)\n",
        "\n",
        "        if MLP_layers == 1 :\n",
        "          layers.append(nn.Linear(dim_embedding, n_tokens_tgt,device=self.device))\n",
        "        else: \n",
        "          #First layer\n",
        "          layers.append(nn.Linear(dim_embedding, dim_embedding*MLP_scale,device=self.device))\n",
        "          layers.append(dropout_l)\n",
        "          layers.append(act)\n",
        "          layers.append(nn.LayerNorm((dim_embedding*MLP_scale),device=self.device))\n",
        "          \n",
        "          for idx in range(MLP_layers-1) :\n",
        "            if idx == MLP_layers-2 : #last hidden layer\n",
        "              layers.append(nn.Linear(dim_embedding*MLP_scale, n_tokens_tgt,device=self.device))\n",
        "            else:\n",
        "              layers.append(nn.Linear(dim_embedding*MLP_scale, dim_embedding*MLP_scale,device=self.device))\n",
        "              layers.append(dropout_l)\n",
        "              layers.append(act)\n",
        "              layers.append(nn.LayerNorm((dim_embedding*MLP_scale),device=self.device))\n",
        "\n",
        "        #Create object _sequential to propagate MLP in one call\n",
        "        self._sequential = nn.Sequential(*layers)\n",
        "        \n",
        "        # PARTIE \"EXPERIMENT\"\n",
        "        self.positional_embeddings_exp = positional_embeddings_exp\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of predictions of the next token distributions in the target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        batch_size = source.shape[0]\n",
        "        seq_len_src, seq_len_tgt = source.shape[1], target.shape[1] \n",
        "\n",
        "        # Embedding & Positional Embedding\n",
        "        src_embedding = self.embedding_src(source)  # [batch, seq_len_src, dim_embedding]\n",
        "\n",
        "        tgt_embedding = self.embedding_tgt(target)\n",
        "\n",
        "        if self.positional_embeddings_exp == True:\n",
        "          tgt_pos = PositionalEncoding_Experiment(self.dim_embedding,seq_len_tgt,device=self.device)\n",
        "          src_pos = PositionalEncoding_Experiment(self.dim_embedding,seq_len_src,device=self.device)\n",
        "          # Embeddings finaux\n",
        "          src = self.dropout_enc(src_embedding + src_pos(source)) # [batch, seq_len_src, dim_embedding]\n",
        "          tgt = self.dropout_enc(tgt_embedding + tgt_pos(target)) # [batch, seq_len_tgt, dim_embedding]\n",
        "\n",
        "        else: \n",
        "          tgt_pos_embedding = torch.arange(0, seq_len_tgt,device=self.device).expand(batch_size,seq_len_tgt) \n",
        "          src_pos_embedding = torch.arange(0, seq_len_src,device=self.device).expand(batch_size,seq_len_src) \n",
        "\n",
        "          # Embeddings finaux\n",
        "          src = self.dropout_enc(src_embedding + self.embedding_pos_src(src_pos_embedding)) # [batch, seq_len_src, dim_embedding]\n",
        "          tgt = self.dropout_enc(tgt_embedding + self.embedding_pos_tgt(tgt_pos_embedding)) # [batch, seq_len_tgt, dim_embedding]\n",
        "          \n",
        "        # Création des masks pour la source et target\n",
        "        src_seq_len = src.shape[1]\n",
        "        tgt_seq_len = tgt.shape[1]\n",
        "        # Masks\n",
        "        tgt_mask = (torch.triu(torch.ones((tgt_seq_len,tgt_seq_len),device=self.device)) == 0).transpose(0, 1)\n",
        "\n",
        "        # Padding masks\n",
        "        src_padding_mask = (source == self.src_pad_idx)\n",
        "        tgt_padding_mask = (target == self.tgt_pad_idx) \n",
        "\n",
        "        # Appel au transformer\n",
        "        if self.torch_fct_transformer[0]: # Utilisation du transformer de Pytorch\n",
        "          output = self.transformer_model(src, tgt, tgt_mask=tgt_mask, \n",
        "                                      src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
        "        else: # Utilisation du transformer \"homemade\"\n",
        "          output = self.transformer_model(src, tgt, tgt_mask_attn=tgt_mask, \n",
        "                                      src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
        "        \n",
        "\n",
        "        return  self._sequential(output)"
      ],
      "metadata": {
        "id": "AGYVF34mvRNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy search\n",
        "\n",
        "Here you have to implement a geedy search to generate a target translation from a trained model and an input source string.\n",
        "The next token will simply be the most probable one."
      ],
      "metadata": {
        "id": "ql6jv2lAK-nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "NB : Greedy Search est en fait cas particulier de Beam Search où beam_width = 1 et max_target = 1\n",
        "\"\"\"\n",
        "\n",
        "def greedy_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        max_sentence_length: int,\n",
        "    ) -> str:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces logits score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentence: The translated source sentence.\n",
        "    \"\"\"\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tokens = torch.LongTensor(src_tokens).to(device)\n",
        "    tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "    target_probs = torch.FloatTensor([1]).to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    with torch.no_grad():\n",
        "      while tgt_tokens.shape[1] < max_sentence_length:\n",
        "        batch_size, n_tokens = tgt_tokens.shape\n",
        "\n",
        "        # Get next tokens\n",
        "        src = einops.repeat(src_tokens, 't -> b t', b=tgt_tokens.shape[0])\n",
        "        predicted = model.forward(src, tgt_tokens)\n",
        "        predicted = torch.softmax(predicted, dim=-1)\n",
        "        probs, predicted = predicted[:, -1].topk(k=1, dim=-1) # On garde le token le plus probable\n",
        "        \n",
        "        tgt_tokens = append_beams(tgt_tokens, predicted) # On l'ajoute à la phrase\n",
        "\n",
        "        if tgt_vocab['<eos>'] in tgt_tokens: # Si on prédit le token '<eo>' alors c'est la fin de phrase\n",
        "          if tgt_tokens.shape[1] < max_sentence_length: # Si la longueur de la phrase est inférieur au minimum requis, on ajoute du padding\n",
        "            padding = torch.zeros((max_sentence_length-tgt_tokens.shape[1], 1), dtype=torch.long, device=device).T\n",
        "            tgt_tokens = torch.cat((tgt_tokens, padding), dim=1)\n",
        "        #print(tgt_tokens)\n",
        "            \n",
        "    for tgt_sentence in tgt_tokens:\n",
        "        tgt_sentence = list(tgt_sentence)[1:]  # Remove <bos> token\n",
        "        tgt_sentence = list(takewhile(lambda t: t != EOS_IDX, tgt_sentence))\n",
        "        tgt_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence))\n",
        "    \n",
        "    sentence = [tgt_sentence]    \n",
        "    sentence = [beautify(s) for s in sentence]\n",
        "\n",
        "    # Join the sentence with its likelihood\n",
        "    sentence = [(s, p.item()) for s, p in zip(sentence, target_probs)]\n",
        "\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "-KMp7piKK905"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam search\n",
        "Beam search is a smarter way of producing a sequence of tokens from\n",
        "an autoregressive model than just using a greedy search.\n",
        "\n",
        "The greedy search always choose the most probable token as the unique\n",
        "and only next target token, and repeat this processus until the *\\<eos\\>* token is predicted.\n",
        "\n",
        "Instead, the beam search selects the k-most probable tokens at each step.\n",
        "From those k tokens, the current sequence is duplicated k times and the k tokens are appended to the k sequences to produce new k sequences.\n",
        "\n",
        "*You don't have to understand this code, but understanding this code once the TP is over could improve your torch tensors skills.*\n",
        "\n",
        "---\n",
        "\n",
        "**More explanations**\n",
        "\n",
        "Since it is done at each step, the number of sequences grows exponentially (k sequences after the first step, k² sequences after the second...).\n",
        "In order to keep the number of sequences low, we remove sequences except the top-s most likely sequences.\n",
        "To do that, we keep track of the likelihood of each sequence.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "Then, we define the likelihood of a target sentence $t = [t_1, ..., t_{N_t}]$ as:\n",
        "\n",
        "$$\n",
        "L(t) = \\prod_{i=1}^{N_t - 1} p(t_{i+1} | s, t_{i}; \\theta )\n",
        "$$\n",
        "\n",
        "Pseudocode of the beam search:\n",
        "```\n",
        "source: [N_s source tokens]  # Shape of [total_source_tokens]\n",
        "target: [1, <bos> token]  # Shape of [n_sentences, current_target_tokens]\n",
        "target_prob: [1]  # Shape of [n_sentences]\n",
        "# We use `n_sentences` as the batch_size dimension\n",
        "\n",
        "while current_target_tokens <= max_target_length:\n",
        "    source = repeat(source, n_sentences)  # Shape of [n_sentences, total_source_tokens]\n",
        "    predicted = model(source, target)[:, -1]  # Predict the next token distributions of all the n_sentences\n",
        "    tokens_idx, tokens_prob = topk(predicted, k)\n",
        "\n",
        "    # Append the `n_sentences * k` tokens to the `n_sentences` sentences\n",
        "    target = repeat(target, k)  # Shape of [n_sentences * k, current_target_tokens]\n",
        "    target = append_tokens(target, tokens_idx)  # Shape of [n_sentences * k, current_target_tokens + 1]\n",
        "\n",
        "    # Update the sentences probabilities\n",
        "    target_prob = repeat(target_prob, k)  # Shape of [n_sentences * k]\n",
        "    target_prob *= tokens_prob\n",
        "\n",
        "    if n_sentences * k >= max_sentences:\n",
        "        target, target_prob = topk_prob(target, target_prob, k=max_sentences)\n",
        "    else:\n",
        "        n_sentences *= k\n",
        "\n",
        "    current_target_tokens += 1\n",
        "```"
      ],
      "metadata": {
        "id": "LgGFG-uXue6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beautify(sentence: str) -> str:\n",
        "    \"\"\"Removes useless spaces.\n",
        "    \"\"\"\n",
        "    punc = {'.', ',', ';'}\n",
        "    for p in punc:\n",
        "        sentence = sentence.replace(f' {p}', p)\n",
        "    \n",
        "    links = {'-', \"'\"}\n",
        "    for l in links:\n",
        "        sentence = sentence.replace(f'{l} ', l)\n",
        "        sentence = sentence.replace(f' {l}', l)\n",
        "    \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "V-GomgGTY2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Q7qcvH2Chp"
      },
      "outputs": [],
      "source": [
        "def indices_terminated(\n",
        "        target: torch.FloatTensor,\n",
        "        eos_token: int\n",
        "    ) -> tuple:\n",
        "    \"\"\"Split the target sentences between the terminated and the non-terminated\n",
        "    sentence. Return the indices of those two groups.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: The sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        eos_token: Value of the End-of-Sentence token.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        terminated: Indices of the terminated sentences (who's got the eos_token).\n",
        "            Shape of [n_terminated, ].\n",
        "        non-terminated: Indices of the unfinished sentences.\n",
        "            Shape of [batch_size-n_terminated, ].\n",
        "    \"\"\"\n",
        "    terminated = [i for i, t in enumerate(target) if eos_token in t]\n",
        "    non_terminated = [i for i, t in enumerate(target) if eos_token not in t]\n",
        "    return torch.LongTensor(terminated), torch.LongTensor(non_terminated)\n",
        "\n",
        "\n",
        "def append_beams(\n",
        "        target: torch.FloatTensor,\n",
        "        beams: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Add the beam tokens to the current sentences.\n",
        "    Duplicate the sentences so one token is added per beam per batch.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: Batch of unfinished sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        beams: Batch of beams for each sentences.\n",
        "            Shape of [batch_size, n_beams].\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        target: Batch of sentences with one beam per sentence.\n",
        "            Shape of [batch_size * n_beams, n_tokens+1].\n",
        "    \"\"\"\n",
        "    batch_size, n_beams = beams.shape\n",
        "    n_tokens = target.shape[1]\n",
        "\n",
        "    target = einops.repeat(target, 'b t -> b c t', c=n_beams)  # [batch_size, n_beams, n_tokens]\n",
        "    beams = beams.unsqueeze(dim=2)  # [batch_size, n_beams, 1]\n",
        "\n",
        "    target = torch.cat((target, beams), dim=2)  # [batch_size, n_beams, n_tokens+1]\n",
        "    target = target.view(batch_size*n_beams, n_tokens+1)  # [batch_size * n_beams, n_tokens+1]\n",
        "    return target\n",
        "\n",
        "\n",
        "def beam_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        beam_width: int,\n",
        "        max_target: int,\n",
        "        max_sentence_length: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces linear score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        beam_width: Number of top-k tokens we keep at each stage.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentences: List of sentences orderer by their likelihood.\n",
        "    \"\"\"\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tokens = torch.LongTensor(src_tokens).to(device)\n",
        "    tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "    target_probs = torch.FloatTensor([1]).to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    with torch.no_grad():\n",
        "        while tgt_tokens.shape[1] < max_sentence_length:\n",
        "            batch_size, n_tokens = tgt_tokens.shape\n",
        "\n",
        "            # Get next beams\n",
        "            src = einops.repeat(src_tokens, 't -> b t', b=tgt_tokens.shape[0])\n",
        "            predicted = model.forward(src, tgt_tokens)\n",
        "            predicted = torch.softmax(predicted, dim=-1)\n",
        "            probs, predicted = predicted[:, -1].topk(k=beam_width, dim=-1)\n",
        "\n",
        "            # Separe between terminated sentences and the others\n",
        "            idx_terminated, idx_not_terminated = indices_terminated(tgt_tokens, EOS_IDX)\n",
        "            idx_terminated, idx_not_terminated = idx_terminated.to(device), idx_not_terminated.to(device)\n",
        "\n",
        "            tgt_terminated = torch.index_select(tgt_tokens, dim=0, index=idx_terminated)\n",
        "            tgt_probs_terminated = torch.index_select(target_probs, dim=0, index=idx_terminated)\n",
        "\n",
        "            filter_t = lambda t: torch.index_select(t, dim=0, index=idx_not_terminated)\n",
        "            tgt_others = filter_t(tgt_tokens)\n",
        "            tgt_probs_others = filter_t(target_probs)\n",
        "            predicted = filter_t(predicted)\n",
        "            probs = filter_t(probs)\n",
        "\n",
        "            # Add the top tokens to the previous target sentences\n",
        "            tgt_others = append_beams(tgt_others, predicted)\n",
        "\n",
        "            # Add padding to terminated target\n",
        "            padd = torch.zeros((len(tgt_terminated), 1), dtype=torch.long, device=device)\n",
        "            tgt_terminated = torch.cat(\n",
        "                (tgt_terminated, padd),\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # Update each target sentence probabilities\n",
        "            tgt_probs_others = torch.repeat_interleave(tgt_probs_others, beam_width)\n",
        "            tgt_probs_others *= probs.flatten()\n",
        "            tgt_probs_terminated *= 0.999  # Penalize short sequences overtime\n",
        "\n",
        "            # Group up the terminated and the others\n",
        "            target_probs = torch.cat(\n",
        "                (tgt_probs_others, tgt_probs_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "            tgt_tokens = torch.cat(\n",
        "                (tgt_others, tgt_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "\n",
        "            # Keep only the top `max_target` target sentences\n",
        "            if target_probs.shape[0] <= max_target:\n",
        "                continue\n",
        "\n",
        "            target_probs, indices = target_probs.topk(k=max_target, dim=0)\n",
        "            tgt_tokens = torch.index_select(tgt_tokens, dim=0, index=indices)\n",
        "\n",
        "    sentences = []\n",
        "    for tgt_sentence in tgt_tokens:\n",
        "        tgt_sentence = list(tgt_sentence)[1:]  # Remove <bos> token\n",
        "        tgt_sentence = list(takewhile(lambda t: t != EOS_IDX, tgt_sentence))\n",
        "        tgt_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence))\n",
        "        sentences.append(tgt_sentence)\n",
        "\n",
        "    sentences = [beautify(s) for s in sentences]\n",
        "\n",
        "    # Join the sentences with their likelihood\n",
        "    sentences = [(s, p.item()) for s, p in zip(sentences, target_probs)]\n",
        "    # Sort the sentences by their likelihood\n",
        "    sentences = [(s, p) for s, p in sorted(sentences, key=lambda k: k[1], reverse=True)]\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVr2FuDcZxC6"
      },
      "source": [
        "# Training loop\n",
        "This is a basic training loop code. It takes a big configuration dictionnary to avoid never ending arguments in the functions.\n",
        "We use [Weights and Biases](https://wandb.ai/) to log the trainings.\n",
        "It logs every training informations and model performances in the cloud.\n",
        "You have to create an account to use it. Every accounts are free for individuals or research teams."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## EXPERIMENT  - METRIC\n",
        "\n",
        "def blue_score(\n",
        "        real_sentence: str,\n",
        "        predict_sentence: str,\n",
        "        max_n: int,\n",
        "    ) -> float:\n",
        "    \"\"\"Compute the blue score accuracy.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        real_sentence: String of the real sentence.\n",
        "        predict_sentence: String of the predicted sentence.\n",
        "        max_n :  the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams, bigrams and trigrams\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "        blue: Scalar of blue score value.\n",
        "    \"\"\"\n",
        "\n",
        "    real_sentence = [real_sentence[:-1].split(\" \")]\n",
        "    predict_sentence = [[predict_sentence[:-1].split(\" \")]]\n",
        "    weights = np.ones(max_n)/max_n\n",
        "\n",
        "    blue = torchtext.data.metrics.bleu_score(real_sentence, predict_sentence, max_n=max_n, weights=weights)\n",
        "    return blue\n",
        "\n",
        "\n",
        "def loop_blue_score(\n",
        "        model   : nn.Module,\n",
        "        config  : dict,\n",
        "        dataset : list,\n",
        "        sentence_idx : list\n",
        "    ) -> list:\n",
        "    \"\"\"Compute the blue score on several sentences.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: trained model\n",
        "        config: Additional parameters from config\n",
        "        dataset : list of tuples of sentences (source (EN), target(FR))\n",
        "        sentence_idx : list of indices \n",
        "    Output\n",
        "    ------\n",
        "        list_blue_score: list of blue score values\n",
        "    \"\"\"\n",
        "    max_n = [1,2,3]\n",
        "    blue = np.zeros(len(max_n))\n",
        "    for i in range(0,len(sentence_idx)):\n",
        "      source, target = dataset[sentence_idx[i]]\n",
        "\n",
        "      if config['search'] == 'beam_search' :\n",
        "          pred, prob = beam_search(\n",
        "            model,\n",
        "            source,\n",
        "            config['src_vocab'],\n",
        "            config['tgt_vocab'],\n",
        "            config['src_tokenizer'],\n",
        "            config['device'],  # It can take a lot of VRAM\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "      elif config['search'] == 'greedy_search' :\n",
        "          pred, prob = greedy_search(\n",
        "            model,\n",
        "            source,\n",
        "            config['src_vocab'],\n",
        "            config['tgt_vocab'],\n",
        "            config['src_tokenizer'],\n",
        "            config['device'],  # It can take a lot of VRAM\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "      \n",
        "      for j in range(0, len(max_n)): \n",
        "        blue[j] += blue_score(target,pred,max_n[j])\n",
        "\n",
        "    blue = blue/len(sentence_idx)\n",
        "    list_blue_score = [(max_n[i], blue[i]) for i in range(0, len(max_n))]\n",
        "    return list_blue_score"
      ],
      "metadata": {
        "id": "JBqzQioqnSrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2I1C8pRXN8j"
      },
      "outputs": [],
      "source": [
        "def print_logs(dataset_type: str, logs: dict):\n",
        "    \"\"\"Print the logs.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        dataset_type: Either \"Train\", \"Eval\", \"Test\" type.\n",
        "        logs: Containing the metric's name and value.\n",
        "    \"\"\"\n",
        "    desc = [\n",
        "        f'{name}: {value:.2f}'\n",
        "        for name, value in logs.items()\n",
        "    ]\n",
        "    desc = '\\t'.join(desc)\n",
        "    desc = f'{dataset_type} -\\t' + desc\n",
        "    desc = desc.expandtabs(5)\n",
        "    print(desc)\n",
        "\n",
        "\n",
        "def topk_accuracy(\n",
        "        real_tokens: torch.FloatTensor,\n",
        "        probs_tokens: torch.FloatTensor,\n",
        "        k: int,\n",
        "        tgt_pad_idx: int,\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Compute the top-k accuracy.\n",
        "    We ignore the PAD tokens.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        real_tokens: Real tokens of the target sentence.\n",
        "            Shape of [batch_size * n_tokens].\n",
        "        probs_tokens: Tokens probability predicted by the model.\n",
        "            Shape of [batch_size * n_tokens, n_target_vocabulary].\n",
        "        k: Top-k accuracy threshold.\n",
        "        src_pad_idx: Source padding index value.\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "        acc: Scalar top-k accuracy value.\n",
        "    \"\"\"\n",
        "    total = (real_tokens != tgt_pad_idx).sum()\n",
        "\n",
        "    _, pred_tokens = probs_tokens.topk(k=k, dim=-1)  # [batch_size * n_tokens, k]\n",
        "    real_tokens = einops.repeat(real_tokens, 'b -> b k', k=k)  # [batch_size * n_tokens, k]\n",
        "\n",
        "    good = (pred_tokens == real_tokens) & (real_tokens != tgt_pad_idx)\n",
        "    acc = good.sum() / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "def loss_batch(\n",
        "        model: nn.Module,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor,\n",
        "        config: dict,\n",
        "    )-> dict:\n",
        "    \"\"\"Compute the metrics associated with this batch.\n",
        "    The metrics are:\n",
        "        - loss\n",
        "        - top-1 accuracy\n",
        "        - top-5 accuracy\n",
        "        - top-10 accuracy\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The model to train.\n",
        "        source: Batch of source tokens.\n",
        "            Shape of [batch_size, n_src_tokens].\n",
        "        target: Batch of target tokens.\n",
        "            Shape of [batch_size, n_tgt_tokens].\n",
        "        config: Additional parameters.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        metrics: Dictionnary containing evaluated metrics on this batch.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    loss_fn = config['loss'].to(device)\n",
        "    metrics = dict()\n",
        "\n",
        "    source, target = source.to(device), target.to(device)\n",
        "    target_in, target_out = target[:, :-1], target[:, 1:]\n",
        "\n",
        "    # Loss\n",
        "    pred = model(source, target_in)  # [batch_size, n_tgt_tokens-1, n_vocab]\n",
        "    pred = pred.view(-1, pred.shape[2])  # [batch_size * (n_tgt_tokens - 1), n_vocab]\n",
        "    target_out = target_out.flatten()  # [batch_size * (n_tgt_tokens - 1),]\n",
        "    metrics['loss'] = loss_fn(pred, target_out)\n",
        "\n",
        "    # Accuracy - we ignore the padding predictions\n",
        "    for k in [1, 5, 10]:\n",
        "        metrics[f'top-{k}'] = topk_accuracy(target_out, pred, k, config['tgt_pad_idx'])\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def eval_model(model: nn.Module, dataloader: DataLoader, config: dict) -> dict:\n",
        "    \"\"\"Evaluate the model on the given dataloader.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    logs = defaultdict(list)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for source, target in dataloader:\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())\n",
        "\n",
        "    for name, values in logs.items():\n",
        "        logs[name] = np.mean(values)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, config: dict):\n",
        "    \"\"\"Train the model in a teacher forcing manner.\n",
        "    \"\"\"\n",
        "    train_loader, val_loader = config['train_loader'], config['val_loader']\n",
        "    train_dataset, val_dataset = train_loader.dataset.dataset, val_loader.dataset.dataset\n",
        "    optimizer = config['optimizer']\n",
        "    clip = config['clip']\n",
        "    device = config['device']\n",
        "\n",
        "    columns = ['epoch']\n",
        "    for mode in ['train', 'validation']:\n",
        "        columns += [\n",
        "            f'{mode} - {colname}'\n",
        "            for colname in ['source', 'target', 'predicted', 'likelihood']\n",
        "        ]\n",
        "    log_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    print(f'Starting training for {config[\"epochs\"]} epochs, using {device}.')\n",
        "    for e in range(config['epochs']):\n",
        "        print(f'\\nEpoch {e+1}')\n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "        logs = defaultdict(list)\n",
        "\n",
        "        for batch_id, (source, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            loss = metrics['loss']\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "        \n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())  # Don't forget the '.item' to free the cuda memory\n",
        "            \n",
        "            if batch_id % config['log_every'] == 0:\n",
        "                for name, value in logs.items():\n",
        "                    logs[name] = np.mean(value)\n",
        "\n",
        "                train_logs = {\n",
        "                    f'Train - {m}': v\n",
        "                    for m, v in logs.items()\n",
        "                }\n",
        "                wandb.log(train_logs)\n",
        "                logs = defaultdict(list)\n",
        "        \n",
        "        # Logs\n",
        "        if len(logs) != 0:\n",
        "            for name, value in logs.items():\n",
        "                logs[name] = np.mean(value)\n",
        "            train_logs = {\n",
        "                f'Train - {m}': v\n",
        "                for m, v in logs.items()\n",
        "            }\n",
        "        else:\n",
        "            logs = {\n",
        "                m.split(' - ')[1]: v\n",
        "                for m, v in train_logs.items()\n",
        "            }\n",
        "\n",
        "        print_logs('Train', logs)\n",
        "\n",
        "        logs = eval_model(model, val_loader, config)\n",
        "        print_logs('Eval', logs)\n",
        "        val_logs = {\n",
        "            f'Validation - {m}': v\n",
        "            for m, v in logs.items()\n",
        "        }\n",
        "\n",
        "        val_source, val_target = val_dataset[ torch.randint(len(val_dataset), (1,)) ]\n",
        "        if config['search'] == 'beam_search' :\n",
        "          val_pred, val_prob = beam_search(\n",
        "              model,\n",
        "              val_source,\n",
        "              config['src_vocab'],\n",
        "              config['tgt_vocab'],\n",
        "              config['src_tokenizer'],\n",
        "              device,  # It can take a lot of VRAM\n",
        "              beam_width=10,\n",
        "              max_target=100,\n",
        "              max_sentence_length=config['max_sequence_length'],\n",
        "          )[0]\n",
        "        elif config['search'] == 'greedy_search' :\n",
        "          val_pred, val_prob = greedy_search(\n",
        "              model,\n",
        "              val_source,\n",
        "              config['src_vocab'],\n",
        "              config['tgt_vocab'],\n",
        "              config['src_tokenizer'],\n",
        "              device,  # It can take a lot of VRAM\n",
        "              max_sentence_length=config['max_sequence_length'],\n",
        "          )[0]\n",
        "          val_prob = None\n",
        "        else :\n",
        "          print (f\"Type of search ({config['search']}) not supported\")\n",
        "\n",
        "        print(val_source)\n",
        "        print(val_pred)\n",
        "        logs = {**train_logs, **val_logs}  # Merge dictionnaries\n",
        "        wandb.log(logs)  # Upload to the WandB cloud\n",
        "\n",
        "        # Table logs\n",
        "        train_source, train_target = train_dataset[ torch.randint(len(train_dataset), (1,)) ]\n",
        "        if config['search'] == 'beam_search' :\n",
        "          train_pred, train_prob = beam_search(\n",
        "              model,\n",
        "              train_source,\n",
        "              config['src_vocab'],\n",
        "              config['tgt_vocab'],\n",
        "              config['src_tokenizer'],\n",
        "              device,  # It can take a lot of VRAM\n",
        "              beam_width=10,\n",
        "              max_target=100,\n",
        "              max_sentence_length=config['max_sequence_length'],\n",
        "          )[0]\n",
        "        elif config['search'] == 'greedy_search' :\n",
        "          train_pred, train_prob = greedy_search(\n",
        "              model,\n",
        "              train_source,\n",
        "              config['src_vocab'],\n",
        "              config['tgt_vocab'],\n",
        "              config['src_tokenizer'],\n",
        "              device,  # It can take a lot of VRAM\n",
        "              max_sentence_length=config['max_sequence_length'],\n",
        "          )[0]\n",
        "          train_prob = None\n",
        "        else :\n",
        "          print (f\"Type of search ({config['search']}) not suported\")\n",
        "\n",
        "        ## Blue Score Train\n",
        "        if config['exp_metric'] == 'blue_score':\n",
        "          train_blue_score = loop_blue_score(model,config,train_dataset, config['train_blue_idx'])\n",
        "          val_blue_score = loop_blue_score(model,config,val_dataset, config['val_blue_idx'])\n",
        "          \n",
        "          blue_train_logs = {\n",
        "                 f'Blue Score ({m}-grams)': v\n",
        "                  for m, v in train_blue_score \n",
        "              }\n",
        "          blue_val_logs = {\n",
        "                 f'Blue Score ({m}-grams)': v\n",
        "                  for m, v in val_blue_score \n",
        "              }\n",
        "          # Print Blue Score\n",
        "          print('\\nExperiment - Metric')\n",
        "          print_logs('Train', blue_train_logs)\n",
        "          print_logs('Eval', blue_val_logs)\n",
        "          \n",
        "          logs = {**blue_train_logs, **blue_val_logs}  # Merge dictionnaries\n",
        "          wandb.log(logs)  # Upload to the WandB cloud\n",
        "        ##\n",
        "\n",
        "        data = [\n",
        "            e + 1,\n",
        "            train_source, train_target, train_pred, train_prob,\n",
        "            val_source, val_target, val_pred, val_prob,\n",
        "        ]\n",
        "        log_table.add_data(*data)\n",
        "    \n",
        "    # Log the table at the end of the training\n",
        "    wandb.log({'Model predictions': log_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the models\n",
        "We can now finally train the models.\n",
        "Choose the right hyperparameters, play with them and try to find\n",
        "ones that lead to good models and good training curves.\n",
        "Try to reach a loss under 1.0.\n",
        "\n",
        "So you know, it is possible to get descent results with approximately 20 epochs.\n",
        "With CUDA enabled, one epoch, even on a big model with a big dataset, shouldn't last more than 10 minutes.\n",
        "A normal epoch is between 1 to 5 minutes.\n",
        "\n",
        "*This is considering Colab Pro, we should try using free Colab to get better estimations.*\n",
        "\n",
        "---\n",
        "\n",
        "To test your implementations, it is easier to try your models\n",
        "in a CPU instance. Indeed, Colab reduces your GPU instances priority\n",
        "with the time you recently past using GPU instances. It would be\n",
        "sad to consume all your GPU time on implementation testing.\n",
        "Moreover, you should try your models on small datasets and with a small number of parameters.\n",
        "For exemple, you could set:\n",
        "```\n",
        "MAX_SEQ_LEN = 10\n",
        "MIN_TOK_FREQ = 20\n",
        "dim_embedding = 40\n",
        "dim_hidden = 60\n",
        "n_layers = 1\n",
        "```\n",
        "\n",
        "You usually don't want to log anything onto WandB when testing your implementation.\n",
        "To deactivate WandB without having to change any line of code, you can type `!wandb offline` in a cell.\n",
        "\n",
        "Once you have rightly implemented the models, you can train bigger models on bigger datasets.\n",
        "When you do this, do not forget to change the runtime as GPU (and use `!wandb online`)!"
      ],
      "metadata": {
        "id": "YImgxCWjlWni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking GPU and logging to wandb\n",
        "\n",
        "!wandb login\n",
        "\n",
        "!nvidia-smi\n",
        "# 0c01e599de3ef5e5a801b3b02166cb76eee87eda : Renaud\n",
        "\n",
        "# b2208215d33eed3434e8409697c7ba75b44bf9e8 : Morgan"
      ],
      "metadata": {
        "id": "WriScTUEsRHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a3d6fe8-e67f-4cb2-d394-68a4dc0e38fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Thu Apr  7 20:29:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate the datasets\n",
        "\n",
        "# MAX_SEQ_LEN = 10    # Original 60\n",
        "# MIN_TOK_FREQ = 20   # Original 2\n",
        "MAX_SEQ_LEN = 60\n",
        "MIN_TOK_FREQ = 2\n",
        "train_dataset, val_dataset = build_datasets(\n",
        "    MAX_SEQ_LEN,\n",
        "    MIN_TOK_FREQ,\n",
        "    en_tokenizer,\n",
        "    fr_tokenizer,\n",
        "    train,\n",
        "    valid,\n",
        ")\n",
        "\n",
        "\n",
        "print(f'English vocabulary size: {len(train_dataset.en_vocab):,}')\n",
        "print(f'French vocabulary size: {len(train_dataset.fr_vocab):,}')\n",
        "\n",
        "print(f'\\nTraining examples: {len(train_dataset):,}')\n",
        "print(f'Validation examples: {len(val_dataset):,}')"
      ],
      "metadata": {
        "id": "iqmpxnO1lgDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac61fa02-eec1-48c7-8c07-f3621cecf813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 11,196\n",
            "French vocabulary size: 16,970\n",
            "\n",
            "Training examples: 173,104\n",
            "Validation examples: 19,235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywFEpplOU5dn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01f2b60-947f-4dbc-b3f6-e11d6ffb9a40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TranslationTransformer                   --                        --\n",
              "├─Embedding: 1-1                         [128, 60, 300]            3,358,800\n",
              "├─Embedding: 1-2                         [128, 60, 300]            5,091,000\n",
              "├─Embedding: 1-3                         [128, 60, 300]            3,358,800\n",
              "├─Dropout: 1-4                           [128, 60, 300]            --\n",
              "├─Embedding: 1-5                         [128, 60, 300]            5,091,000\n",
              "├─Dropout: 1-6                           [128, 60, 300]            --\n",
              "├─Transformer: 1-7                       [128, 60, 300]            --\n",
              "│    └─TransformerEncoder: 2-1           [128, 60, 300]            --\n",
              "│    │    └─LayerNorm: 3-1               [128, 60, 300]            600\n",
              "│    └─TransformerDecoder: 2-2           [128, 60, 300]            --\n",
              "│    │    └─LayerNorm: 3-2               [128, 60, 300]            600\n",
              "├─Sequential: 1-8                        [128, 60, 16970]          --\n",
              "│    └─Linear: 2-3                       [128, 60, 1800]           541,800\n",
              "│    └─Dropout: 2-4                      [128, 60, 1800]           --\n",
              "│    └─ELU: 2-5                          [128, 60, 1800]           --\n",
              "│    └─LayerNorm: 2-6                    [128, 60, 1800]           3,600\n",
              "│    └─Linear: 2-7                       [128, 60, 16970]          30,562,970\n",
              "==========================================================================================\n",
              "Total params: 48,009,170\n",
              "Trainable params: 48,009,170\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 6.15\n",
              "==========================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1374.41\n",
              "Params size (MB): 192.04\n",
              "Estimated Total Size (MB): 1566.57\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 25,          # Original 5\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'search' : 'beam_search', #'greedy_search'\n",
        "    #'search' : 'greedy_search',\n",
        "    'exp_metric' : None, # 'blue_score' or None\n",
        "    'train_blue_idx' : [torch.randint(len(train_dataset), (1,)) for i in range(0,20)],\n",
        "    'val_blue_idx': [torch.randint(len(val_dataset), (1,)) for i in range(0,20)],\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    # 'dim_embedding': 40,  # Original 196\n",
        "    # 'dim_hidden': 60,     # Original 256\n",
        "    # 'n_layers': 3,        # Original 3\n",
        "    'dim_embedding': 300,  # Original 196\n",
        "    'dim_hidden': 256,     # Original 256\n",
        "    'n_layers': 3,        # Original 3\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'RNN',  # A modifier\n",
        "    'torch_fct_translation': False,  # A modifier\n",
        "    'MLP_param_RNN_GRU' : ['LeakyReLU01',1,1], #Activation fct (\"LeakyReLU01\",\"ELU\",\"Mish\"), number of layers in the MLP, scale of hidden layer\n",
        "    'MLP_param_transformer' : ['ELU',2,6], #Activation fct (\"LeakyReLU01\",\"ELU\",\"Mish\"), number of layers in the MLP, scale of hidden layer\n",
        "    'torch_fct_transformer' : [False, False, False, False], # Utiliser Pytorch pour [Transformer, Encoder, Decoder, MultiheadAttention]\n",
        "    'positional_embeddings_exp' : False, # Mettre True si on veut tester d'autres positional Embdeddings dans le Transformer\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\"\"\"\n",
        "model = TranslationRNN(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        "    config['model_type'],\n",
        "    config['torch_fct_translation'],\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        "    config['torch_fct_transformer'],\n",
        "    config['positional_embeddings_exp']\n",
        ")\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maOTVtk4acxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "887833a7fa0042c3854967bb07913331",
            "ca96f0e36f0642039181c9b0014bab29",
            "796dfa48de9e4767a45c662832647c82",
            "67540fd8274c481eb4894f5f09147c58",
            "37a89d43532e429188a0c0fca892dbeb",
            "3f7d1805426745b39080b3fe6098b1f2",
            "4a1b393ad3a94d22b2b1dc70735c8d89",
            "d0ec8155dace48c28c66467e4c085dd4"
          ]
        },
        "outputId": "67f410ae-c020-43bf-db70-29e2228114ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online, running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.13"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220407_221333-2vyz2qam</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Final%20Test/runs/2vyz2qam\" target=\"_blank\">fast-waterfall-1</a></strong> to <a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Final%20Test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 25 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.76     top-1: 0.48    top-5: 0.68    top-10: 0.75\n",
            "Eval -    loss: 2.57     top-1: 0.50    top-5: 0.71    top-10: 0.77\n",
            "Are we allowed to take pictures here?\n",
            "Est-ce que nous allons ici ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.17     top-1: 0.55    top-5: 0.77    top-10: 0.83\n",
            "Eval -    loss: 2.03     top-1: 0.58    top-5: 0.79    top-10: 0.84\n",
            "The traffic light changed to red.\n",
            "Le dîner s'est familier.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.84     top-1: 0.58    top-5: 0.81    top-10: 0.87\n",
            "Eval -    loss: 1.79     top-1: 0.62    top-5: 0.82    top-10: 0.87\n",
            "We can't trust Tom anymore.\n",
            "Nous ne pouvons plus confiance à Tom.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.43     top-1: 0.67    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.66     top-1: 0.64    top-5: 0.84    top-10: 0.89\n",
            "Is he breathing?\n",
            "Est-ce qu'il ?\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.35     top-1: 0.67    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.57     top-1: 0.66    top-5: 0.86    top-10: 0.90\n",
            "Our company's showroom was a hit with the ladies.\n",
            "Notre entreprise a été valeur d'enfance.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.38     top-1: 0.67    top-5: 0.87    top-10: 0.92\n",
            "Eval -    loss: 1.52     top-1: 0.67    top-5: 0.87    top-10: 0.90\n",
            "Tom doesn't blame you for anything.\n",
            "Tom ne vous reproche rien.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.30     top-1: 0.68    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.48     top-1: 0.68    top-5: 0.87    top-10: 0.91\n",
            "Some people think that it is difficult for a native speaker of English to learn Chinese, but I disagree.\n",
            "Certaines personnes pensent que ça ne pense qu'une langue maternelle.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.14     top-1: 0.71    top-5: 0.90    top-10: 0.95\n",
            "Eval -    loss: 1.46     top-1: 0.69    top-5: 0.88    top-10: 0.91\n",
            "I asked what he was going to do.\n",
            "J'ai demandé ce qu'il allait faire.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.13     top-1: 0.71    top-5: 0.91    top-10: 0.95\n",
            "Eval -    loss: 1.44     top-1: 0.69    top-5: 0.88    top-10: 0.91\n",
            "This watch is a real bargain.\n",
            "Cette montre est une bonne affaire.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.96     top-1: 0.72    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 1.43     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "I thought I was being smart.\n",
            "Je pensais que j'étais intelligente.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.08     top-1: 0.72    top-5: 0.90    top-10: 0.95\n",
            "Eval -    loss: 1.41     top-1: 0.70    top-5: 0.89    top-10: 0.92\n",
            "Everyone is very proud of you.\n",
            "Tout le monde est très fier.\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 0.93     top-1: 0.75    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 1.40     top-1: 0.70    top-5: 0.89    top-10: 0.92\n",
            "I have a big house.\n",
            "J'ai une grande maison.\n",
            "\n",
            "Epoch 13\n",
            "Train -   loss: 1.11     top-1: 0.71    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 1.39     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "It's important that I hear this.\n",
            "C'est important que j'entendre ça.\n",
            "\n",
            "Epoch 14\n",
            "Train -   loss: 0.97     top-1: 0.74    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 1.38     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "This one is the worst.\n",
            "Celle-ci est la pire.\n",
            "\n",
            "Epoch 15\n",
            "Train -   loss: 0.93     top-1: 0.74    top-5: 0.94    top-10: 0.97\n",
            "Eval -    loss: 1.38     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "I want you to be prepared.\n",
            "Je veux que tu sois préparé.\n",
            "\n",
            "Epoch 16\n",
            "Train -   loss: 0.84     top-1: 0.76    top-5: 0.94    top-10: 0.97\n",
            "Eval -    loss: 1.37     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "The door's locked.\n",
            "La porte est verrouillée.\n",
            "\n",
            "Epoch 17\n",
            "Train -   loss: 0.84     top-1: 0.76    top-5: 0.94    top-10: 0.97\n",
            "Eval -    loss: 1.37     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "You have our respect.\n",
            "Tu as notre respect.\n",
            "\n",
            "Epoch 18\n",
            "Train -   loss: 0.77     top-1: 0.76    top-5: 0.95    top-10: 0.98\n",
            "Eval -    loss: 1.37     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "She decided to resign her job.\n",
            "Elle décida de démissionner son emploi.\n",
            "\n",
            "Epoch 19\n",
            "Train -   loss: 0.72     top-1: 0.78    top-5: 0.96    top-10: 0.98\n",
            "Eval -    loss: 1.37     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "I play with my son every night.\n",
            "Je joue avec mon fils avec tous les nuits.\n",
            "\n",
            "Epoch 20\n",
            "Train -   loss: 0.80     top-1: 0.77    top-5: 0.95    top-10: 0.97\n",
            "Eval -    loss: 1.36     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "That was never our intention.\n",
            "Ce n'était jamais notre intention.\n",
            "\n",
            "Epoch 21\n",
            "Train -   loss: 0.73     top-1: 0.79    top-5: 0.95    top-10: 0.98\n",
            "Eval -    loss: 1.35     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "He is often late for school.\n",
            "Il est souvent en retard à l'école.\n",
            "\n",
            "Epoch 22\n",
            "Train -   loss: 0.68     top-1: 0.81    top-5: 0.96    top-10: 0.98\n",
            "Eval -    loss: 1.35     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Is money important to you?\n",
            "L'argent est important pour vous ?\n",
            "\n",
            "Epoch 23\n",
            "Train -   loss: 0.70     top-1: 0.80    top-5: 0.96    top-10: 0.98\n",
            "Eval -    loss: 1.35     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "How did you get here so fast?\n",
            "Comment êtes-vous arrivée ici si rapidement ?\n",
            "\n",
            "Epoch 24\n",
            "Train -   loss: 0.68     top-1: 0.80    top-5: 0.97    top-10: 0.98\n",
            "Eval -    loss: 1.35     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "This looks like a trap.\n",
            "Ça a l'air d'être un piège.\n",
            "\n",
            "Epoch 25\n",
            "Train -   loss: 0.79     top-1: 0.78    top-5: 0.95    top-10: 0.98\n",
            "Eval -    loss: 1.35     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "He can drive a car.\n",
            "Il peut conduire une voiture.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.277 MB of 0.277 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "887833a7fa0042c3854967bb07913331"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▆▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▃▄▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇███████████</td></tr><tr><td>Train - top-10</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇▇▇▇█▇██████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇███████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▃▅▅▆▆▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▆▇▇▇▇▇▇██████████████</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▆▇▇▇▇▇▇▇█████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.79233</td></tr><tr><td>Train - top-1</td><td>0.78063</td></tr><tr><td>Train - top-10</td><td>0.97631</td></tr><tr><td>Train - top-5</td><td>0.94933</td></tr><tr><td>Validation - loss</td><td>1.3487</td></tr><tr><td>Validation - top-1</td><td>0.7285</td></tr><tr><td>Validation - top-10</td><td>0.93023</td></tr><tr><td>Validation - top-5</td><td>0.90208</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">fast-waterfall-1</strong>: <a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Final%20Test/runs/2vyz2qam\" target=\"_blank\">https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Final%20Test/runs/2vyz2qam</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220407_221333-2vyz2qam/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!wandb online  # online / offline to activate or deactivate WandB logging\n",
        "\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3 - Transformer Final Test',  # Title of your project\n",
        "        group='Transformer - Final model',  # In what group of runs do you want this run to be in? \n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PFIyvKUefdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c11b2d-2370-44e8-851d-0781f3e07a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. (58.20621%) \t Je suis trop fatiguée pour marcher.\n",
            "1. (29.44964%) \t Je suis trop fatigué pour marcher.\n",
            "2. (2.80672%) \t Je suis trop fatiguée pour continuer.\n",
            "3. (0.75601%) \t Je suis trop fatiguée pour continuer à marcher.\n",
            "4. (0.36998%) \t J'trop fatiguée pour marcher.\n",
            "5. (0.31987%) \t Je suis trop fatiguée pour marcher fatigué.\n",
            "6. (0.19011%) \t J'ai trop fatiguée pour marcher.\n",
            "7. (0.15915%) \t Je vais trop fatiguée pour marcher.\n",
            "8. (0.13635%) \t Je suis trop fatiguée pour continuer de marcher.\n",
            "9. (0.13133%) \t Je suis trop fatigués pour marcher.\n"
          ]
        }
      ],
      "source": [
        "#sentence = \"It is possible to try your work here.\"\n",
        "sentence = \"I'm too tired to walk.\"\n",
        "\n",
        "preds = beam_search(\n",
        "    model,\n",
        "    sentence,\n",
        "    config['src_vocab'],\n",
        "    config['tgt_vocab'],\n",
        "    config['src_tokenizer'],\n",
        "    config['device'],\n",
        "    beam_width=10,\n",
        "    max_target=100,\n",
        "    max_sentence_length=config['max_sequence_length']\n",
        ")[:10]\n",
        "\n",
        "for i, (translation, likelihood) in enumerate(preds):\n",
        "    print(f'{i}. ({likelihood*100:.5f}%) \\t {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n"
      ],
      "metadata": {
        "id": "uHhixEEGzWRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "1. Explain the differences between Vanilla RNN, GRU-RNN, and Transformers. \n",
        "\n",
        "First of all, here's a recap of the mathematical concepts used in vanilla RNN and GRU. \n",
        "\n",
        "RNN Description from : https://pytorch.org/docs/master/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN) \\\\\n",
        "For each element in the input sequence, each layer computes the following\n",
        "\n",
        "$$h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})$$\n",
        "\n",
        "where : $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$.\n",
        "\n",
        "GRU Description from : https://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#GRU \\\\\n",
        "\n",
        "For each element in the input sequence, each layer computes the following\n",
        "\n",
        "$$r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
        "z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
        "n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
        "h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}$$\n",
        "\n",
        "where :$h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $r_t$, $z_t$, $n_t$ are the reset, update, and new gates, respectively. $\\sigma$ is the sigmoid function, and $*$ is the element-wise product.\n",
        "\n",
        "As we can see, on the contrary to RNN, GRU uses a concept of gates and we'll see why in our explanation below :\n",
        "\n",
        "- RNN : \n",
        "  - One problem with vanilla RNN is that it faces a short-term memory problem. This is due to the \"vanishing gradient\" and gradient explosion. Indeed, during backpropagation the gradient is used to update the weights but it depends on the influence of the previous layer. If the previous gradient is small, the next one will be even smaller. (For the exploding gradient, it is the contrary.). Thus with a small gradient, the effect on the weights' update will be low or will have no effect at all. Therefore, it has an impact on the learning capability of the model.\n",
        "\n",
        "- GRU:\n",
        "\n",
        "  - GRU inherits of the structure of RNNs. However, it adds different gates to balance the hidden states. The major difference with vanilla RNN is GRU's ability to update a memory cell using the R (reset), Z (update) and N (new gate) gates. The R gate allows to reset the state of the cell. The Z gate allows to update the state of the cell and N allows to create a new temporary output which considers the previous hidden layer and the value of R. We then obtain an output h which is a linear combination of Z and N. GRU is a much more flexible model, thus approaching LSTM. This has the benefit of increasing the memory capacity of the model. Indeed, it is capable of forgetting or focusing on previous/current hidden states and the input.\n",
        "However, if the number of GRU cells is too high, it is still possible to face the \"vanishing gradient\" problem. Thus, the GRU is not always very good at retaining context. This leads us to concept of \"attention\" for Transformers.\n",
        "\n",
        "- Transformers:\n",
        "  - Finally, in a Transformer, sentences are processed entirely rather than word by word. By doing so, there is no longer the risk of losing past information as was the case with previous architectures.\n",
        "Moreover, as mentioned before, the \"Attention\" mechanism allows the Transformer to compute similarity scores between words in a sentence and thus give the model information about the relationships between words to know which words to focus on.\n",
        "As we will see in the next question, there is also the need to add positional information of the words as their not processed sequentially. "
      ],
      "metadata": {
        "id": "lE6uysqYw3yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YTcaAAPGxFqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "2. Why is positionnal encoding necessary in Transformers and not in RNNs?\n",
        "\n",
        "The RNN/GRU intrinsically take into account the word order. Indeed, for an input sentence, words enter one by one in a sequential way in the RNN allowing to take into account their positional information.\n",
        "\n",
        "However with a Transformer, the input is not sequential words but the whole sequence is directly introduced in the model. Then the \"Attention\" concept allows to tell the model where to focus but there is then no information about the position. This is why positional embeddings are necessary for the Transformer : it gives positional information of each word to the model. \n"
      ],
      "metadata": {
        "id": "utooH5kyw-rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eJOi0kJTxCy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\n",
        "3. Describe the preprocessing process. Detail how the initial dataset is processed before being fed to the translation models.\n",
        "\n",
        "First, we download the data and create a dataframe with the sentences in English and the corresponding sentences in French. Then, we split the data as follows: 90% training data, 10% validation data thanks to the function train_test_split().\n",
        "Afterwards, we use the function get_tokenizer() to transform words of the sentences into tokens and we also define special tokens such as :    \n",
        "- unk --> for an unknown word\n",
        "- pad --> token for padding\n",
        "- bos --> token for the beggining of sentence \n",
        "- eos --> token for the end of sentence \n",
        "\n",
        "Then, here's how the data are preprocessed through the call of the function build_datasets():\n",
        "1. We use the \"preprocess\" function in order to filter the dataset : \n",
        "      - preprocess() removes the break line tag ('\\n') and removes from the dataset the examples that contain at least one sentence whose length exceeds the set limit (for memory reasons).\n",
        "2. We use the \"build_vocab\" function to create vocabularies based on the sample of sentence that we kept:\n",
        "      - build_vocab() allows to build vocabularies (english and french) with a minimum occurrence condition on words to be integrated in the vocabulary.  It also adds the token \"unk\" for unknown words. \n",
        "\n",
        "3. We use \"TranslationDataset\" class to tokenize each sentence, ddd start(\"bos\")/end(\"eos\") tokens and put the results in an \"english\" tensor and a \"french\" tensor:\n",
        "    - The function __getitem__() tokenizes sentences and adds start-of-sentence (\"bos\") and end-of-sentence (\"eos\") tokens for each of the sentences and saves it in two tensors (1 English, 1 French)\n",
        "    \n",
        "Finally, in the model configuration (\"config\" dictionary), we use the generate_batch() function to add padding so that each sentence of a batch has the same length. "
      ],
      "metadata": {
        "id": "cEFiNIdoxJrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Q3cF7bPxNQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Small report - experiments\n",
        "Once everything is working fine, you can explore and do some little research work.\n",
        "\n",
        "For exemple, you can experiment with the hyperparameters.\n",
        "What are the effect of the differents hyperparameters with the final model performance? What about training time?\n",
        "\n",
        "What are some other metrics you could have for machine translation? Can you compute them and add them to your WandB report?\n",
        "\n",
        "Those are only examples, you can do whatever you think will be interesting.\n",
        "This part account for many points, *feel free to go wild!*\n",
        "\n",
        "---\n",
        "*Make a small report about your experiments here.*"
      ],
      "metadata": {
        "id": "Y3tQdusIjPCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our experiment plan\n",
        "To improve our models, we performed several tests to assess and tune hyper-parameters and methods used for the translation. We decided to perform the following tests : \n",
        "\n",
        "-----\n",
        "#### EXPERIMENT 1 : RNN/GRU comparison (2 tests)\n",
        "We wanted to compare RNN and GRU with the following parameters: \n",
        "  - Basic Parameter, 10 epochs, fully connected output.\n",
        "-----\n",
        "\n",
        "#### EXPERIMENT 2 : Tuning of the MLP (multilayer perceptron) for RNN/GRU (9 tests)\n",
        "We performed a fine tuning of the MLP for the best model obtained in the first test. We did the following tests:\n",
        "  - Comparison of the activation functions : leakyReLU, ELU, Mish - with MLP (4 layers, scale 1) on 5 epochs (3 tests)\n",
        "  - With the best activation function, tuning of the MLP on 3 epochs: (6 tests)\n",
        "    - Scale : 2, 6 and 8\n",
        "    - Number of layers : 2 and 3\n",
        "-----\n",
        "\n",
        "### EXPERIMENT 3 : Transformer's hyper-parameters (7 tests)\n",
        "We performed tests on the transformer to tune the hyperparameters with a fully connected output : \n",
        "- Comparison on the batch size: 64, 128, 256 - 5 epochs\n",
        "- Comparison on the dimension of the embeddings: 100, 200, 300 - 5 epochs\n",
        "\n",
        "We assumed that we need to fine a balance on the embedding dimension. On one hand, not enough embeddings will lead to a small space of embedding and thus bad translations. On the other hand, to many embedding dimensions will lead to a too large space and the model will probably struggle to create coherent translations.\n",
        "\n",
        "- Comparison on the positional embedding initialization - 5 epoch\n",
        "\n",
        "We wanted to compare the classic positional embedding (with nn.Embeddding) with another method to initiliaze them. (see class PositionalEncoding_Experiment)\n",
        "\n",
        "-----\n",
        "### EXPERIMENT 4 : Tuning of the MLP for the Transformer (4 tests)\n",
        "We performed a tuning of the transformer's MLP with the best parameters determined before. As you we'll see in the following tests reports, those best parameters are :    \n",
        "- Batch size :  128 \n",
        "- Dim Embedding : 300 \n",
        "- Positional Embedding : Classic\n",
        "- Activation function : ELU\n",
        "\n",
        "On 5 epochs, tuning of the MLP : (6 tests)\n",
        "- Scale : 6 and 8\n",
        "- Number of layers : 2 and 3\n",
        "\n",
        "-----\n",
        "### EXPERIMENT 5 : Compare Greedy and Beam Search and Blue score (Transformer) (2 tests)\n",
        "Here, we wanted to compare the Greedy and Beam Search methods. As we expected and because Greedy Search is a special case of Beam Search (with beam_width=1 and max_target=1), Beam Search is better to produce a coherent translation.\n",
        "We also introduced another metric to assess our translated sentences through the training and validation. \n",
        "\n",
        "BLUE SCORE :    \n",
        "Blue score is a metric for evaluating a generated sentence to a reference sentence. A perfect match results in a score of 1.0. It works by counting matching n-grams in the predicted sentence to n-grams in the reference text, where 1-gram would be each token and a bigram comparison would be each word pair, etc. The comparison is made regardless of word order.\n",
        "\n",
        "-----\n",
        "### FINAL RUN : Best model (Transformer)\n",
        "Thanks to all the previous tests, we were able to set our best model. The output can be seen above. \n",
        "We ran it on 25 epochs, it took 1h35 (~3min50 per epoch) with the following parameters : \n",
        "- Batch size :  128 \n",
        "- Dim Embedding : 300 \n",
        "- Positional Embedding : Classic\n",
        "- Activation function : ELU\n",
        "- MLP : Scale = 6, Layers = 2\n",
        "- Beam Search\n",
        "-----\n",
        "You will find below our results for each tests and our analysis."
      ],
      "metadata": {
        "id": "Z8RbmRrmjfgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<a href=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/RNN-vs-GRU--VmlldzoxODEwNjg5\" target=\"_blank\" > Experiment 1 - RNN vs GRU </a> </br>\n",
        "<a href=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/GRU-Tuning-of-the-MLP--VmlldzoxODEwODEy\" target=\"_blank\" > Experiment 2 - MLP Tuning (GRU) </a></br>\n",
        "<a href=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/Transformer-Hyperparameter-Tuning--VmlldzoxODEyNDQ0\" target=\"_blank\" > Experiment 3 - Transformer s hyperparameters </a></br>\n",
        "<a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer/reports/Transformer-MLP-tuning--VmlldzoxODA1NzUx\" target=\"_blank\" > Experiment 4 - Transformer MLP Tuning </a></br>\n",
        "<a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Greedy%20vs%20Beam/reports/Transformer-Greedy-vs-Beam-Search--VmlldzoxODA3NDAz\" target=\"_blank\" > Experiment 5 - Greedy vs Beam Search and BLUE score </a></br>\n",
        "<a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Final%20Test/reports/Transformer-Final-test--VmlldzoxODA3NDM0\" target=\"_blank\" > Final Run : Best model (Transformer) </a>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "DoCngNF8BW_x",
        "outputId": "cc305b18-2a01-4c87-8a80-3a993df5257e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/RNN-vs-GRU--VmlldzoxODEwNjg5\" target=\"_blank\" > Experiment 1 - RNN vs GRU </a> </br>\n",
              "<a href=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/GRU-Tuning-of-the-MLP--VmlldzoxODEwODEy\" target=\"_blank\" > Experiment 2 - MLP Tuning (GRU) </a></br>\n",
              "<a href=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/Transformer-Hyperparameter-Tuning--VmlldzoxODEyNDQ0\" target=\"_blank\" > Experiment 3 - Transformer s hyperparameters </a></br>\n",
              "<a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer/reports/Transformer-MLP-tuning--VmlldzoxODA1NzUx\" target=\"_blank\" > Experiment 4 - Transformer MLP Tuning </a></br>\n",
              "<a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Greedy%20vs%20Beam/reports/Transformer-Greedy-vs-Beam-Search--VmlldzoxODA3NDAz\" target=\"_blank\" > Experiment 5 - Greedy vs Beam Search and BLUE score </a></br>\n",
              "<a href=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Final%20Test/reports/Transformer-Final-test--VmlldzoxODA3NDM0\" target=\"_blank\" > Final Run : Best model (Transformer) </a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 1 - RNN vs GRU"
      ],
      "metadata": {
        "id": "KcNaWIsBv9hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/RNN-vs-GRU--VmlldzoxODEwNjg5\" style=\"border:none;height:1024px;width:100%\">"
      ],
      "metadata": {
        "id": "Qk1kHMG4wDG-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba2d72e1-eb1c-4e4e-894a-75e659624677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/RNN-vs-GRU--VmlldzoxODEwNjg5\" style=\"border:none;height:1024px;width:100%\">"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 2 - MLP Tuning (GRU)"
      ],
      "metadata": {
        "id": "WRVg5Y8MwD0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/GRU-Tuning-of-the-MLP--VmlldzoxODEwODEy\" style=\"border:none;height:1024px;width:100%\">"
      ],
      "metadata": {
        "id": "tVRsqjIgwIfx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33f06b44-43a5-4437-ce63-b97988e027b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/GRU-Tuning-of-the-MLP--VmlldzoxODEwODEy\" style=\"border:none;height:1024px;width:100%\">"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 3 - Transformer's hyperparameters"
      ],
      "metadata": {
        "id": "0ZPF045TwK3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/Transformer-Hyperparameter-Tuning--VmlldzoxODEyNDQ0\" style=\"border:none;height:1024px;width:100%\">"
      ],
      "metadata": {
        "id": "6P1D21fowPyi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fcb5153-c494-4f8c-fca8-39c96767e6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/renaudlesperance/INF8225%20-%20TP3%20-%20Final%20Run/reports/Transformer-Hyperparameter-Tuning--VmlldzoxODEyNDQ0\" style=\"border:none;height:1024px;width:100%\">"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 4 - Transformer MLP Tuning"
      ],
      "metadata": {
        "id": "erf-bi6in6fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer/reports/Transformer-MLP-tuning--VmlldzoxODA1NzUx\" style=\"border:none;height:1024px;width:100%\">"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sKavQ2vFn6FO",
        "outputId": "188175bb-3842-4f97-e8a8-f918c44d74e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer/reports/Transformer-MLP-tuning--VmlldzoxODA1NzUx\" style=\"border:none;height:1024px;width:100%\">"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 5 - Greedy vs Beam Search and BLUE score"
      ],
      "metadata": {
        "id": "L7JmnBZ9nqUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Greedy%20vs%20Beam/reports/Transformer-Greedy-vs-Beam-Search--VmlldzoxODA3NDAz\" style=\"border:none;height:1024px;width:100%\">"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-_1oH8jenqBj",
        "outputId": "daf2e049-d487-43db-e99f-3396e64202de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Greedy%20vs%20Beam/reports/Transformer-Greedy-vs-Beam-Search--VmlldzoxODA3NDAz\" style=\"border:none;height:1024px;width:100%\">"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Run : Best model (Transformer)"
      ],
      "metadata": {
        "id": "xY7t_UpLniQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Final%20Test/reports/Transformer-Final-test--VmlldzoxODA3NDM0\" style=\"border:none;height:1024px;width:100%\">"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1h_RiHV6m9jm",
        "outputId": "a2384838-63ae-4a9b-b91d-d04d33cb1f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/morgiizz/INF8225%20-%20TP3%20-%20Transformer%20Final%20Test/reports/Transformer-Final-test--VmlldzoxODA3NDM0\" style=\"border:none;height:1024px;width:100%\">"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vLbVbH4lu4J0",
        "KcNaWIsBv9hI",
        "WRVg5Y8MwD0-",
        "0ZPF045TwK3m",
        "erf-bi6in6fd",
        "L7JmnBZ9nqUy",
        "xY7t_UpLniQd"
      ],
      "name": "INF8225_TP3_LESPERANCE_PEJU.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "887833a7fa0042c3854967bb07913331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca96f0e36f0642039181c9b0014bab29",
              "IPY_MODEL_796dfa48de9e4767a45c662832647c82"
            ],
            "layout": "IPY_MODEL_67540fd8274c481eb4894f5f09147c58"
          }
        },
        "ca96f0e36f0642039181c9b0014bab29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37a89d43532e429188a0c0fca892dbeb",
            "placeholder": "​",
            "style": "IPY_MODEL_3f7d1805426745b39080b3fe6098b1f2",
            "value": "0.419 MB of 0.419 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "796dfa48de9e4767a45c662832647c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a1b393ad3a94d22b2b1dc70735c8d89",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0ec8155dace48c28c66467e4c085dd4",
            "value": 1
          }
        },
        "67540fd8274c481eb4894f5f09147c58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37a89d43532e429188a0c0fca892dbeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f7d1805426745b39080b3fe6098b1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a1b393ad3a94d22b2b1dc70735c8d89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0ec8155dace48c28c66467e4c085dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}